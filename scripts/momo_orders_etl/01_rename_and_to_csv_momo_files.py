#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
01_rename_and_to_csv_momo_files.py

åŠŸèƒ½ï¼š
- éè¿´æƒæ data_raw/momo ä¸‹ .xls/.xlsx/.csvï¼ˆè·³é backup/ï¼‰
- åŒæ™‚æ”¯æ´èˆŠ/æ–°æª”åæ ¼å¼è§£æ
- è½‰å‡ºç‚ºæ–°å‘½åè¦å‰‡çš„ .csvï¼ˆæ‰€æœ‰æ¬„ä½ä»¥å­—ä¸²è™•ç†ï¼‰
- è‹¥ç›®æ¨™ .csv å·²å­˜åœ¨ï¼šåƒ…ä¿ç•™è¼ƒæ–°çš„ç‰ˆæœ¬ï¼ˆä»¥ä¿®æ”¹æ™‚é–“åˆ¤æ–·ï¼‰
- è½‰æª”æˆåŠŸå¾Œå°‡ä¾†æº .xls/.xlsx ä¾æ–°å‘½åè¦å‰‡é‡æ–°å‘½åæ¬åˆ° backup/ï¼›
  è‹¥ backup å…§åŒåå·²å­˜åœ¨ï¼šåƒ…ä¿ç•™è¼ƒæ–°çš„ç‰ˆæœ¬ï¼ˆä»¥ä¿®æ”¹æ™‚é–“åˆ¤æ–·ï¼‰
"""

import re
import csv
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, Tuple, Optional

import pandas as pd
import shutil
import os

# ===== åƒæ•¸è¨­å®š =====
INPUT_CSV_ENCODING = "utf-8-sig"
INPUT_CSV_SEP = ","
INPUT_CSV_KEEP_DEFAULT_NA = False
INPUT_CSV_NA_FILTER = False

OUTPUT_CSV_ENCODING = "utf-8-sig"
OUTPUT_CSV_QUOTING = csv.QUOTE_ALL
OUTPUT_CSV_LINETERMINATOR = "\n"

EXCEL_SHEET_STRATEGY = "first"  # 'first' æˆ– 'concat'


# ===== æ—¥èªŒ =====
def setup_logging(project_root: Path) -> None:
    log_dir = project_root / 'logs'
    log_dir.mkdir(exist_ok=True)
    for log_file in log_dir.glob('rename_momo_files_*.log'):
        try:
            log_file.unlink()
        except OSError as e:
            print(f"ç„¡æ³•åˆªé™¤èˆŠæ—¥èªŒ {log_file}: {e}")

    log_filename = f'rename_momo_files_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'
    log_path = log_dir / log_filename

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_path, 'w', 'utf-8'),
            logging.StreamHandler()
        ]
    )


# ===== æ¨¡çµ„è³‡è¨Š =====
def get_module_info() -> Dict[str, Dict[str, Dict[str, str] or str]]:
    return {
        'A1102': {
            'name': 'æœªå‡ºè²¨è¨‚å–®ç®¡ç†',
            'delivery_methods': {
                '1': 'å» å•†é…é€',
                '2': 'è¶…å•†å–è²¨',
                '3': 'ç¬¬ä¸‰æ–¹ç‰©æµ',
            }
        },
        'C1105': {
            'name': 'å°å¸³è¨‚å–®æ˜ç´°',
            'delivery_methods': {}
        }
    }


# ===== æ–°å‘½ååˆ¤æ–· =====
def is_already_renamed(stem: str, module_info: Dict[str, Dict[str, str]]) -> bool:
    a1102_delivery_names = '|'.join(map(re.escape, module_info['A1102']['delivery_methods'].values()))
    pat_a1102 = rf'^A1102_[123]_({a1102_delivery_names})_[0-9]{{6}}_\d{{8}}_\d{{6}}$'

    c1105_name = re.escape(module_info['C1105']['name'])
    pat_c1105 = rf'^C1105_{c1105_name}_[0-9]{{6}}_\d{{8}}_\d{{6}}$'

    return bool(re.match(pat_a1102, stem) or re.match(pat_c1105, stem))


# ===== æª”åè§£æï¼ˆèˆŠ/æ–°æ ¼å¼ï¼‰=====
def parse_filename(stem: str) -> Optional[Tuple[str, str, str, str, str]]:
    name = stem.strip()

    # èˆŠæ ¼å¼ A1102ï¼šA1102_<delivery>_<ignored>_<cust6>_<dt14 | date8_time6>(å°¾å¯å« (n)/(å…¨å½¢) èˆ‡ç©ºç™½)
    m = re.match(
        r'^(A1102)_(\d)_(\d+)_([0-9]{6})_(\d{14}|\d{8}_\d{6})(?:\s*[\(\ï¼ˆ]\d+[\)\ï¼‰])?$',
        name
    )
    if m:
        module_code, delivery_code, customer_code, dt = m.group(1), m.group(2), m.group(4), m.group(5)
        if '_' in dt:
            date_str, time_str = dt.split('_', 1)
        else:
            date_str, time_str = dt[:8], dt[8:]
        return module_code, delivery_code, customer_code, date_str, time_str

    # èˆŠæ ¼å¼ C1105ï¼šC1105_<cust6>_<dt14 | date8_time6>(å°¾å¯å« (n)/(å…¨å½¢) èˆ‡ç©ºç™½)
    m = re.match(
        r'^(C1105)_([0-9]{6})_(\d{14}|\d{8}_\d{6})(?:\s*[\(\ï¼ˆ]\d+[\)\ï¼‰])?$',
        name
    )
    if m:
        module_code, customer_code, dt = m.group(1), m.group(2), m.group(3)
        delivery_code = ''
        if '_' in dt:
            date_str, time_str = dt.split('_', 1)
        else:
            date_str, time_str = dt[:8], dt[8:]
        return module_code, delivery_code, customer_code, date_str, time_str

    # æ–°æ ¼å¼ A1102ï¼šA1102_<delivery>_<é…é€ä¸­æ–‡å>_<cust6>_<YYYYMMDD>_<HHMMSS>
    m = re.match(
        r'^(A1102)_(\d)_[^_]+_([0-9]{6})_(\d{8})_(\d{6})$',
        name
    )
    if m:
        return m.group(1), m.group(2), m.group(3), m.group(4), m.group(5)

    # æ–°æ ¼å¼ C1105ï¼šC1105_å°å¸³è¨‚å–®æ˜ç´°_<cust6>_<YYYYMMDD>_<HHMMSS>
    m = re.match(
        r'^(C1105)_å°å¸³è¨‚å–®æ˜ç´°_([0-9]{6})_(\d{8})_(\d{6})$',
        name
    )
    if m:
        return m.group(1), '', m.group(2), m.group(3), m.group(4)

    return None


# ===== ç”¢ç”Ÿæ–°æª”åï¼ˆä¸å«å‰¯æª”åï¼‰=====
def generate_new_stem(module_code: str, delivery_code: str, customer_code: str,
                      date_str: str, time_str: str,
                      module_info: Dict[str, Dict[str, str]]) -> str:
    if module_code not in module_info:
        return f"{module_code}_{delivery_code}_{customer_code}_{date_str}_{time_str}"

    module_name = module_info[module_code]['name']
    if delivery_code and delivery_code in module_info[module_code].get('delivery_methods', {}):
        delivery_name = module_info[module_code]['delivery_methods'][delivery_code]
        return f"{module_code}_{delivery_code}_{delivery_name}_{customer_code}_{date_str}_{time_str}"
    else:
        return f"{module_code}_{module_name}_{customer_code}_{date_str}_{time_str}"


# ===== è®€å¯«ï¼ˆå…¨å­—ä¸²ï¼‰=====
def read_file_as_str_df(path: Path) -> pd.DataFrame:
    suffix = path.suffix.lower()
    if suffix in ('.xls', '.xlsx'):
        if EXCEL_SHEET_STRATEGY == 'concat':
            sheets = pd.read_excel(path, sheet_name=None, dtype=str)
            dfs = []
            for sheet_name, df in sheets.items():
                df = df.astype(str).fillna("")
                df.insert(0, "_sheet", str(sheet_name))
                dfs.append(df)
            return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()
        else:
            return pd.read_excel(path, sheet_name=0, dtype=str).astype(str).fillna("")
    elif suffix == '.csv':
        return pd.read_csv(
            path, dtype=str, sep=INPUT_CSV_SEP,
            encoding=INPUT_CSV_ENCODING,
            keep_default_na=INPUT_CSV_KEEP_DEFAULT_NA,
            na_filter=INPUT_CSV_NA_FILTER
        ).astype(str).fillna("")
    else:
        raise ValueError(f"ä¸æ”¯æ´çš„å‰¯æª”å: {suffix}")


def write_df_to_csv_all_str(df: pd.DataFrame, out_path: Path) -> None:
    df = df.copy()
    
    # è™•ç†å¸³å‹™æ•¸å­—æ¬„ä½ï¼Œç¢ºä¿å°æ•¸é»ä¸‹å…©ä½
    cost_fields = ['product_cost_untaxed', 'platform_product_cost', 'product_original_price']
    for field in cost_fields:
        if field in df.columns:
            # è½‰æ›ç‚ºæ•¸å€¼ï¼Œä¿ç•™å°æ•¸é»ä¸‹å…©ä½
            df[field] = pd.to_numeric(df[field], errors='coerce').round(2)
    
    # å…¶ä»–æ¬„ä½è½‰ç‚ºå­—ä¸²
    df = df.astype(str).fillna("")
    out_path.parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(
        out_path,
        index=False,
        quoting=OUTPUT_CSV_QUOTING,
        encoding=OUTPUT_CSV_ENCODING,
        lineterminator=OUTPUT_CSV_LINETERMINATOR
    )


# ===== æ¯”è¼ƒä¸¦ä¿ç•™è¼ƒæ–°ç‰ˆæœ¬ =====
def keep_newer_when_conflict(src: Path, dst: Path) -> str:
    """
    ç›®æ¨™ dst å·²å­˜åœ¨æ™‚ï¼š
    - è‹¥ src è¼ƒæ–°ï¼šè¦†å¯« dstï¼Œå›å‚³ 'replaced'
    - è‹¥ dst è¼ƒæ–°ï¼šä¿ç•™ dstï¼Œåˆªé™¤/ä¸Ÿæ£„ srcï¼Œå›å‚³ 'kept_dst'
    - è‹¥ dst ä¸å­˜åœ¨ï¼šå›å‚³ 'moved'ï¼ˆå‘¼å«ç«¯åŸ·è¡Œ move/å¯«å‡ºï¼‰
    å‚™è¨»ï¼šä¾æª”æ¡ˆ mtime åˆ¤æ–·æ–°èˆŠã€‚
    """
    if not dst.exists():
        return 'moved'
    try:
        src_mtime = src.stat().st_mtime
    except FileNotFoundError:
        # src å¯èƒ½æ˜¯æš«å­˜å°šæœªå¯«å‡ºï¼ˆä¾‹å¦‚ DataFrame è¦å¯«å‡ºçš„ CSV é‚„æ²’å­˜åœ¨æª”æ¡ˆç³»çµ±ï¼‰
        # å‘¼å«ç«¯æ‡‰è‡ªè¡Œè™•ç†æ­¤æƒ…å¢ƒ
        return 'dst_exists'
    dst_mtime = dst.stat().st_mtime
    if src_mtime > dst_mtime:
        # src è¼ƒæ–° â†’ è¦†å¯«
        try:
            dst.unlink()
        except Exception:
            pass
        return 'replaced'
    else:
        return 'kept_dst'


# ===== ä¸»æµç¨‹ =====
def process_files(momo_dir: Path, module_info: Dict[str, Dict[str, str]]) -> None:
    backup_dir = momo_dir / "backup"
    backup_dir.mkdir(exist_ok=True)

    # éè¿´æƒæï¼Œä½†è·³é backup ç›®éŒ„
    targets = [
        p for p in momo_dir.rglob("*")
        if p.is_file()
        and p.suffix.lower() in ('.xls', '.xlsx', '.csv')
        and backup_dir not in p.parents
    ]

    if not targets:
        logging.warning(f"åœ¨ {momo_dir} ç›®éŒ„ä¸‹æ²’æœ‰æ‰¾åˆ°ä»»ä½•æª”æ¡ˆ")
        return

    logging.info(f"æ‰¾åˆ° {len(targets)} å€‹å¾…è™•ç†æª”æ¡ˆ")
    converted_count = skipped_count = moved_count = replaced_csv = kept_csv = replaced_backup = kept_backup = 0

    for src_path in targets:
        stem, ext = src_path.stem, src_path.suffix.lower()
        logging.info(f"è™•ç†æª”æ¡ˆ: {src_path.name}")

        # å·²ç‚ºæ–°å‘½åä¸”ç‚º CSV â†’ ç›´æ¥è·³é
        if ext == '.csv' and is_already_renamed(stem, module_info):
            logging.info(f"å·²ç‚ºæ–°å‘½åä¸”ç‚º CSVï¼Œç•¥é: {src_path.name}")
            skipped_count += 1
            continue

        parsed = parse_filename(stem)
        if not parsed:
            logging.warning(f"ç„¡æ³•è§£ææª”åï¼Œç•¥é: {stem}")
            skipped_count += 1
            continue

        module_code, delivery_code, customer_code, date_str, time_str = parsed
        new_stem = generate_new_stem(module_code, delivery_code, customer_code, date_str, time_str, module_info)

        # 1) ç”¢å‡º CSVï¼ˆä¿ç•™è¼ƒæ–°ç‰ˆæœ¬ï¼‰
        out_csv = src_path.parent / f"{new_stem}.csv"
        if out_csv.exists():
            # æ¯”è¼ƒä¾†æºèˆ‡ç¾æœ‰ CSV çš„ mtime
            decision = keep_newer_when_conflict(src_path, out_csv)
            if decision == 'kept_dst':
                logging.info(f"ç›®æ¨™ CSV è¼ƒæ–°ï¼Œä¿ç•™ç¾æœ‰ï¼š{out_csv.name}ï¼›ä¾†æºç•¥éï¼š{src_path.name}")
                kept_csv += 1
            else:
                # 'replaced' or 'moved'ï¼ˆè¦–ç‚ºéœ€è¦é‡å¯«ï¼‰
                try:
                    df = read_file_as_str_df(src_path)
                    write_df_to_csv_all_str(df, out_csv)  # è¦†å¯«æˆ–å¯«å…¥
                    if decision == 'replaced':
                        logging.info(f"ğŸ” è¦†å¯«è¼ƒèˆŠ CSVï¼š{out_csv.name}")
                        replaced_csv += 1
                    else:
                        logging.info(f"âœ… è½‰æª”æˆåŠŸï¼š{src_path.name} -> {out_csv.name}")
                        converted_count += 1
                except Exception as e:
                    logging.error(f"âŒ è½‰æª”å¤±æ•—: {src_path.name} - {e}")
                    skipped_count += 1
                    continue
        else:
            # ä¸å­˜åœ¨ â†’ ç›´æ¥å¯«å‡º
            try:
                df = read_file_as_str_df(src_path)
                write_df_to_csv_all_str(df, out_csv)
                logging.info(f"âœ… è½‰æª”æˆåŠŸï¼š{src_path.name} -> {out_csv.name}")
                converted_count += 1
            except Exception as e:
                logging.error(f"âŒ è½‰æª”å¤±æ•—: {src_path.name} - {e}")
                skipped_count += 1
                continue

        # 2) Excel æ¬åˆ° backupï¼ˆä¿ç•™è¼ƒæ–°ç‰ˆæœ¬ï¼‰
        if ext in ('.xls', '.xlsx'):
            dst_backup = backup_dir / f"{new_stem}{ext}"
            if dst_backup.exists():
                decision = keep_newer_when_conflict(src_path, dst_backup)
                if decision == 'kept_dst':
                    # ç›®çš„è¼ƒæ–° â†’ ä¿ç•™ç›®çš„ï¼›åˆªé™¤ä¾†æºé¿å…é‡è¤‡
                    try:
                        src_path.unlink()
                        logging.info(f"ğŸ—‘ï¸ ä¾†æºè¼ƒèˆŠï¼Œåˆªé™¤ä¾†æºï¼š{src_path.name}ï¼›ä¿ç•™ backupï¼š{dst_backup.name}")
                        kept_backup += 1
                    except Exception as e:
                        logging.error(f"åˆªé™¤èˆŠä¾†æºå¤±æ•—ï¼š{src_path.name} - {e}")
                        skipped_count += 1
                elif decision in ('replaced', 'moved'):
                    # ç›®çš„è¼ƒèˆŠæˆ–ä¸å­˜åœ¨ â†’ ç”¨æ–°ä¾†æºè¦†å¯«/æ¬ç§»
                    try:
                        dst_backup.unlink(missing_ok=True)
                    except TypeError:
                        # Python <3.8 ç„¡ missing_ok
                        if dst_backup.exists():
                            try:
                                dst_backup.unlink()
                            except Exception:
                                pass
                    try:
                        shutil.move(str(src_path), str(dst_backup))
                        if decision == 'replaced':
                            logging.info(f"ğŸ” è¦†å¯«è¼ƒèˆŠ backupï¼š{dst_backup.name}")
                            replaced_backup += 1
                        else:
                            logging.info(f"ğŸ“¦ æ¬ç§»è‡³ backupï¼š{dst_backup.name}")
                            moved_count += 1
                    except Exception as e:
                        logging.error(f"âŒ æ¬ç§»/è¦†å¯« backup å¤±æ•—ï¼š{src_path.name} - {e}")
                        skipped_count += 1
            else:
                # ç›®çš„æª”ä¸å­˜åœ¨ â†’ ç›´æ¥æ¬ç§»
                try:
                    shutil.move(str(src_path), str(dst_backup))
                    logging.info(f"ğŸ“¦ æ¬ç§»è‡³ backupï¼š{dst_backup.name}")
                    moved_count += 1
                except Exception as e:
                    logging.error(f"âŒ æ¬ç§»è‡³ backup å¤±æ•—ï¼š{src_path.name} - {e}")
                    skipped_count += 1

    logging.info("\n=== ä½œæ¥­å®Œæˆ ===")
    logging.info(f"æˆåŠŸè½‰å‡º CSVï¼š{converted_count} å€‹")
    logging.info(f"è¦†å¯«è¼ƒèˆŠ CSVï¼š{replaced_csv} å€‹ï¼›ä¿ç•™è¼ƒæ–° CSVï¼š{kept_csv} å€‹")
    logging.info(f"æ¬ç§»è‡³ backupï¼š{moved_count} å€‹ï¼›è¦†å¯« backupï¼š{replaced_backup} å€‹ï¼›ä¿ç•™è¼ƒæ–° backupï¼š{kept_backup} å€‹")
    logging.info(f"è·³éï¼š{skipped_count} å€‹")


# ===== å…¥å£ =====
def main():
    script_dir = Path(__file__).parent
    project_root = script_dir.parents[1]
    setup_logging(project_root)
    logging.info("=== Momo æª”æ¡ˆé‡æ–°å‘½åèˆ‡è½‰æª”é–‹å§‹ ===")

    momo_dir = project_root / 'data_raw' / 'momo'
    if not momo_dir.exists():
        logging.error(f"æ‰¾ä¸åˆ° momo ç›®éŒ„: {momo_dir}")
        return

    module_info = get_module_info()
    process_files(momo_dir, module_info)
    logging.info("=== Momo æª”æ¡ˆé‡æ–°å‘½åèˆ‡è½‰æª”å®Œæˆ ===")


if __name__ == '__main__':
    main()
