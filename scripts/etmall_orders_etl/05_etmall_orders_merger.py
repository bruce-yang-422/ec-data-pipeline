#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ±æ£®è³¼ç‰©è¨‚å–®åˆä½µè…³æœ¬
åˆä½µéŠ·å”®å ±è¡¨å’Œå‡ºè²¨å ±è¡¨ï¼Œä»¥éŠ·å”®å ±è¡¨ç‚ºä¸»æª”ï¼Œorder_line_uid ç‚º key åŒ¹é…
"""

import os
import sys
import pandas as pd
import logging
from pathlib import Path
from datetime import datetime
import glob

# è¨­å®šå°ˆæ¡ˆæ ¹ç›®éŒ„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(project_root / 'logs' / 'etmall_orders_merger.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

def find_latest_merged_files(temp_dir: Path) -> tuple:
    """
    å°‹æ‰¾æœ€æ–°çš„åˆä½µæª”æ¡ˆ
    
    Args:
        temp_dir: è‡¨æ™‚ç›®éŒ„
        
    Returns:
        tuple: (éŠ·å”®å ±è¡¨æª”æ¡ˆè·¯å¾‘, å‡ºè²¨å ±è¡¨æª”æ¡ˆè·¯å¾‘)
    """
    sales_report_pattern = str(temp_dir / "etmall_sales_report_merged_*.csv")
    shipping_orders_pattern = str(temp_dir / "etmall_shipping_orders_merged_*.csv")
    
    # å°‹æ‰¾éŠ·å”®å ±è¡¨æª”æ¡ˆ
    sales_files = glob.glob(sales_report_pattern)
    if not sales_files:
        logging.error(f"æ‰¾ä¸åˆ°éŠ·å”®å ±è¡¨æª”æ¡ˆï¼š{sales_report_pattern}")
        return None, None
    
    # å°‹æ‰¾å‡ºè²¨å ±è¡¨æª”æ¡ˆ
    shipping_files = glob.glob(shipping_orders_pattern)
    if not shipping_files:
        logging.error(f"æ‰¾ä¸åˆ°å‡ºè²¨å ±è¡¨æª”æ¡ˆï¼š{shipping_orders_pattern}")
        return None, None
    
    # é¸æ“‡æœ€æ–°çš„æª”æ¡ˆï¼ˆæŒ‰æª”æ¡ˆåç¨±ä¸­çš„æ™‚é–“æˆ³ï¼‰
    latest_sales_file = max(sales_files, key=lambda x: Path(x).stem.split('_')[-1])
    latest_shipping_file = max(shipping_files, key=lambda x: Path(x).stem.split('_')[-1])
    
    logging.info(f"æ‰¾åˆ°æœ€æ–°éŠ·å”®å ±è¡¨æª”æ¡ˆï¼š{Path(latest_sales_file).name}")
    logging.info(f"æ‰¾åˆ°æœ€æ–°å‡ºè²¨å ±è¡¨æª”æ¡ˆï¼š{Path(latest_shipping_file).name}")
    
    return latest_sales_file, latest_shipping_file

def load_and_validate_files(sales_file: str, shipping_file: str) -> tuple:
    """
    è¼‰å…¥ä¸¦é©—è­‰æª”æ¡ˆ
    
    Args:
        sales_file: éŠ·å”®å ±è¡¨æª”æ¡ˆè·¯å¾‘
        shipping_file: å‡ºè²¨å ±è¡¨æª”æ¡ˆè·¯å¾‘
        
    Returns:
        tuple: (éŠ·å”®å ±è¡¨DataFrame, å‡ºè²¨å ±è¡¨DataFrame)
    """
    try:
        # è¼‰å…¥éŠ·å”®å ±è¡¨
        logging.info("è¼‰å…¥éŠ·å”®å ±è¡¨...")
        sales_df = pd.read_csv(sales_file, encoding='utf-8-sig', dtype=str)
        logging.info(f"éŠ·å”®å ±è¡¨è¼‰å…¥æˆåŠŸï¼š{len(sales_df)} ç­†ï¼Œ{len(sales_df.columns)} æ¬„ä½")
        
        # è¼‰å…¥å‡ºè²¨å ±è¡¨
        logging.info("è¼‰å…¥å‡ºè²¨å ±è¡¨...")
        shipping_df = pd.read_csv(shipping_file, encoding='utf-8-sig', dtype=str)
        logging.info(f"å‡ºè²¨å ±è¡¨è¼‰å…¥æˆåŠŸï¼š{len(shipping_df)} ç­†ï¼Œ{len(shipping_df.columns)} æ¬„ä½")
        
        # æª¢æŸ¥å¿…è¦çš„æ¬„ä½
        if 'order_line_uid' not in sales_df.columns:
            logging.error("éŠ·å”®å ±è¡¨ç¼ºå°‘ order_line_uid æ¬„ä½")
            return None, None
            
        if 'order_line_uid' not in shipping_df.columns:
            logging.error("å‡ºè²¨å ±è¡¨ç¼ºå°‘ order_line_uid æ¬„ä½")
            return None, None
        
        return sales_df, shipping_df
        
    except Exception as e:
        logging.error(f"è¼‰å…¥æª”æ¡ˆå¤±æ•—ï¼š{str(e)}")
        return None, None

def merge_orders(sales_df: pd.DataFrame, shipping_df: pd.DataFrame, output_dir: Path) -> str:
    """
    åˆä½µè¨‚å–®è³‡æ–™
    
    Args:
        sales_df: éŠ·å”®å ±è¡¨DataFrame
        shipping_df: å‡ºè²¨å ±è¡¨DataFrame
        output_dir: è¼¸å‡ºç›®éŒ„
        
    Returns:
        str: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
    """
    logging.info("é–‹å§‹åˆä½µè¨‚å–®è³‡æ–™...")
    
    # å»ºç«‹è¼¸å‡ºç›®éŒ„
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ä»¥éŠ·å”®å ±è¡¨ç‚ºä¸»æª”ï¼Œè¤‡è£½æ‰€æœ‰æ¬„ä½
    merged_df = sales_df.copy()
    logging.info(f"ä¸»æª”ï¼ˆéŠ·å”®å ±è¡¨ï¼‰è³‡æ–™ç­†æ•¸ï¼š{len(merged_df)} ç­†")
    
    # æ‰¾å‡ºéŠ·å”®å ±è¡¨ä¸­ç‚ºç©ºå€¼çš„æ¬„ä½
    empty_columns = []
    for col in merged_df.columns:
        if merged_df[col].isna().all() or (merged_df[col] == '').all():
            empty_columns.append(col)
    
    logging.info(f"ç™¼ç¾ç©ºå€¼æ¬„ä½ï¼š{len(empty_columns)} å€‹")
    if empty_columns:
        logging.info(f"ç©ºå€¼æ¬„ä½åˆ—è¡¨ï¼š{empty_columns[:10]}...")  # åªé¡¯ç¤ºå‰10å€‹
    
    # æª¢æŸ¥å‡ºè²¨å ±è¡¨ä¸­æ˜¯å¦æœ‰é€™äº›ç©ºå€¼æ¬„ä½
    available_columns = []
    for col in empty_columns:
        if col in shipping_df.columns:
            available_columns.append(col)
    
    logging.info(f"å‡ºè²¨å ±è¡¨ä¸­å¯ç”¨çš„æ¬„ä½ï¼š{len(available_columns)} å€‹")
    if available_columns:
        logging.info(f"å¯ç”¨æ¬„ä½åˆ—è¡¨ï¼š{available_columns[:10]}...")  # åªé¡¯ç¤ºå‰10å€‹
    
    # å»ºç«‹å‡ºè²¨å ±è¡¨çš„è¤‡åˆç´¢å¼•ï¼ˆä»¥ è¨‚å–®è™Ÿç¢¼ + é …æ¬¡ ç‚º keyï¼‰
    shipping_df['composite_key'] = shipping_df['è¨‚å–®è™Ÿç¢¼'].astype(str) + '_' + shipping_df['è¨‚å–®é …æ¬¡'].astype(str)
    shipping_indexed = shipping_df.set_index('composite_key')
    logging.info(f"å‡ºè²¨å ±è¡¨è¤‡åˆç´¢å¼•å»ºç«‹å®Œæˆï¼š{len(shipping_indexed)} ç­†")
    
    # çµ±è¨ˆåŒ¹é…æƒ…æ³
    matched_count = 0
    filled_count = 0
    
    # é¡¯ç¤ºä¸€äº›ç¯„ä¾‹é€²è¡Œæ¯”è¼ƒ
    sales_sample = merged_df['order_line_uid'].head(5).tolist()
    shipping_sample = list(shipping_indexed.index[:5])
    logging.info(f"éŠ·å”®å ±è¡¨ order_line_uid ç¯„ä¾‹ï¼š{sales_sample}")
    logging.info(f"å‡ºè²¨å ±è¡¨è¤‡åˆç´¢å¼•ç¯„ä¾‹ï¼š{shipping_sample}")
    
    # é€ç­†è™•ç†éŠ·å”®å ±è¡¨è³‡æ–™
    for idx, row in merged_df.iterrows():
        order_sn = str(row['order_sn'])
        item_no = str(row['item_no'])
        composite_key = f"{order_sn}_{item_no}"
        
        # æª¢æŸ¥å‡ºè²¨å ±è¡¨ä¸­æ˜¯å¦æœ‰å°æ‡‰çš„è¤‡åˆç´¢å¼•
        if composite_key in shipping_indexed.index:
            matched_count += 1
            shipping_row = shipping_indexed.loc[composite_key]
            
            # å¡«è£œç©ºå€¼æ¬„ä½
            for col in available_columns:
                if pd.isna(merged_df.at[idx, col]) or merged_df.at[idx, col] == '':
                    if not pd.isna(shipping_row[col]) and shipping_row[col] != '':
                        merged_df.at[idx, col] = shipping_row[col]
                        filled_count += 1
    
    # åˆ†æè¨‚å–®è™Ÿç¢¼ç¯„åœ
    sales_order_sns = set(merged_df['order_sn'].astype(str))
    shipping_order_sns = set(shipping_df['è¨‚å–®è™Ÿç¢¼'].astype(str))
    
    # æ‰¾å‡ºé‡ç–Šçš„è¨‚å–®è™Ÿç¢¼
    overlapping_orders = sales_order_sns.intersection(shipping_order_sns)
    logging.info(f"éŠ·å”®å ±è¡¨è¨‚å–®è™Ÿç¢¼ç¯„åœï¼š{min(sales_order_sns)} - {max(sales_order_sns)}")
    logging.info(f"å‡ºè²¨å ±è¡¨è¨‚å–®è™Ÿç¢¼ç¯„åœï¼š{min(shipping_order_sns)} - {max(shipping_order_sns)}")
    logging.info(f"é‡ç–Šçš„è¨‚å–®è™Ÿç¢¼ï¼š{len(overlapping_orders)} å€‹")
    
    if overlapping_orders:
        logging.info(f"é‡ç–Šè¨‚å–®ç¯„ä¾‹ï¼š{list(overlapping_orders)[:5]}")
    
    logging.info(f"åŒ¹é…æˆåŠŸçš„è¨‚å–®ï¼š{matched_count} ç­†")
    logging.info(f"å¡«è£œçš„ç©ºå€¼ï¼š{filled_count} å€‹")
    
    # è™•ç† item_no æ ¼å¼ï¼ˆå€‹ä½æ•¸å‰é¢è£œ0ï¼‰
    if 'item_no' in merged_df.columns:
        merged_df['item_no'] = merged_df['item_no'].astype(str).str.zfill(2)
        logging.info("å·²è™•ç† item_no æ ¼å¼ï¼ˆå€‹ä½æ•¸å‰é¢è£œ0ï¼‰")
    
    # æŒ‰ç…§ order_sn å’Œ item_no æ’åº
    if 'order_sn' in merged_df.columns and 'item_no' in merged_df.columns:
        # è½‰æ›ç‚ºæ•¸å€¼å‹æ…‹é€²è¡Œæ’åº
        merged_df['order_sn_numeric'] = pd.to_numeric(merged_df['order_sn'], errors='coerce')
        merged_df['item_no_numeric'] = pd.to_numeric(merged_df['item_no'], errors='coerce')
        
        # æ’åº
        merged_df = merged_df.sort_values(['order_sn_numeric', 'item_no_numeric'], ascending=[True, True])
        
        # ç§»é™¤è‡¨æ™‚æ¬„ä½
        merged_df = merged_df.drop(['order_sn_numeric', 'item_no_numeric'], axis=1)
        
        logging.info("å·²æŒ‰ç…§ order_sn å’Œ item_no æ’åºï¼ˆç”±å°åˆ°å¤§ï¼‰")
    
    # ç”Ÿæˆè¼¸å‡ºæª”æ¡ˆåç¨±
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_filename = f"etmall_orders_merged_{timestamp}.csv"
    output_path = output_dir / output_filename
    
    # å„²å­˜åˆä½µå¾Œçš„è³‡æ–™
    try:
        merged_df.to_csv(output_path, index=False, encoding='utf-8-sig')
        logging.info(f"âœ… åˆä½µå®Œæˆï¼š{output_filename}")
        logging.info(f"   è¼¸å‡ºä½ç½®ï¼š{output_path}")
        logging.info(f"   æœ€çµ‚è³‡æ–™ç­†æ•¸ï¼š{len(merged_df)} ç­†")
        logging.info(f"   æ¬„ä½æ•¸é‡ï¼š{len(merged_df.columns)} å€‹")
        
        return str(output_path)
        
    except Exception as e:
        logging.error(f"å„²å­˜æª”æ¡ˆå¤±æ•—ï¼š{str(e)}")
        return None

def cleanup_input_files(sales_file: str, shipping_file: str):
    """
    æ¸…ç†è¼¸å…¥æª”æ¡ˆ
    
    Args:
        sales_file: éŠ·å”®å ±è¡¨æª”æ¡ˆè·¯å¾‘
        shipping_file: å‡ºè²¨å ±è¡¨æª”æ¡ˆè·¯å¾‘
    """
    deleted_count = 0
    files_to_delete = [Path(sales_file), Path(shipping_file)]
    
    for file_path in files_to_delete:
        try:
            if file_path.exists():
                file_path.unlink()
                logging.info(f"å·²åˆªé™¤è¼¸å…¥æª”æ¡ˆï¼š{file_path.name}")
                deleted_count += 1
        except Exception as e:
            logging.warning(f"åˆªé™¤æª”æ¡ˆå¤±æ•—ï¼š{file_path.name} - {str(e)}")
    
    logging.info(f"ç¸½å…±åˆªé™¤ {deleted_count} å€‹è¼¸å…¥æª”æ¡ˆ")

def cleanup_old_merged_files(temp_dir: Path, keep_latest: bool = True):
    """
    æ¸…ç† temp\\etmall ç›®éŒ„ä¸‹çš„èˆŠåˆä½µæª”æ¡ˆï¼Œåªä¿ç•™æœ€æ–°çš„
    
    Args:
        temp_dir: temp\\etmall ç›®éŒ„è·¯å¾‘
        keep_latest: æ˜¯å¦ä¿ç•™æœ€æ–°çš„æª”æ¡ˆ
    """
    if not temp_dir.exists():
        logging.warning(f"ç›®éŒ„ä¸å­˜åœ¨ï¼š{temp_dir}")
        return
    
    # å°‹æ‰¾æ‰€æœ‰åˆä½µæª”æ¡ˆ
    merged_files = []
    
    # å°‹æ‰¾éŠ·å”®å ±è¡¨åˆä½µæª”æ¡ˆ
    sales_files = list(temp_dir.glob("etmall_sales_report_merged_*.csv"))
    merged_files.extend(sales_files)
    
    # å°‹æ‰¾å‡ºè²¨å ±è¡¨åˆä½µæª”æ¡ˆ
    shipping_files = list(temp_dir.glob("etmall_shipping_orders_merged_*.csv"))
    merged_files.extend(shipping_files)
    
    # å°‹æ‰¾è¨‚å–®åˆä½µæª”æ¡ˆ
    order_files = list(temp_dir.glob("etmall_orders_merged_*.csv"))
    merged_files.extend(order_files)
    
    if not merged_files:
        logging.info("æ²’æœ‰æ‰¾åˆ°éœ€è¦æ¸…ç†çš„åˆä½µæª”æ¡ˆ")
        return
    
    # æŒ‰ä¿®æ”¹æ™‚é–“æ’åºï¼Œæœ€æ–°çš„åœ¨æœ€å¾Œ
    merged_files.sort(key=lambda x: x.stat().st_mtime)
    
    if keep_latest:
        # ä¿ç•™æœ€æ–°çš„æª”æ¡ˆ
        files_to_delete = merged_files[:-1]  # é™¤äº†æœ€å¾Œä¸€å€‹ï¼ˆæœ€æ–°çš„ï¼‰
        files_to_keep = merged_files[-1:]    # åªä¿ç•™æœ€æ–°çš„
    else:
        # åˆªé™¤æ‰€æœ‰æª”æ¡ˆ
        files_to_delete = merged_files
        files_to_keep = []
    
    deleted_count = 0
    for file_path in files_to_delete:
        try:
            if file_path.exists():
                file_path.unlink()
                logging.info(f"å·²åˆªé™¤èˆŠæª”æ¡ˆï¼š{file_path.name}")
                deleted_count += 1
        except Exception as e:
            logging.warning(f"åˆªé™¤æª”æ¡ˆå¤±æ•—ï¼š{file_path.name} - {str(e)}")
    
    if files_to_keep:
        logging.info(f"ä¿ç•™æœ€æ–°æª”æ¡ˆï¼š{files_to_keep[0].name}")
    
    logging.info(f"ç¸½å…±åˆªé™¤ {deleted_count} å€‹èˆŠåˆä½µæª”æ¡ˆ")

def main():
    """ä¸»å‡½æ•¸"""
    logging.info("=" * 50)
    logging.info("é–‹å§‹åŸ·è¡Œæ±æ£®è³¼ç‰©è¨‚å–®åˆä½µè…³æœ¬")
    logging.info("=" * 50)
    
    # è¨­å®šè·¯å¾‘
    temp_dir = project_root / "temp" / "etmall"
    output_dir = project_root / "temp" / "etmall"
    
    logging.info(f"å°ˆæ¡ˆæ ¹ç›®éŒ„ï¼š{project_root}")
    logging.info(f"è‡¨æ™‚ç›®éŒ„ï¼š{temp_dir}")
    logging.info(f"è¼¸å‡ºç›®éŒ„ï¼š{output_dir}")
    
    # å°‹æ‰¾æœ€æ–°çš„åˆä½µæª”æ¡ˆ
    sales_file, shipping_file = find_latest_merged_files(temp_dir)
    
    if not sales_file or not shipping_file:
        logging.error("æ‰¾ä¸åˆ°å¿…è¦çš„åˆä½µæª”æ¡ˆï¼Œç¨‹å¼çµæŸ")
        return
    
    # è¼‰å…¥ä¸¦é©—è­‰æª”æ¡ˆ
    sales_df, shipping_df = load_and_validate_files(sales_file, shipping_file)
    
    if sales_df is None or shipping_df is None:
        logging.error("æª”æ¡ˆè¼‰å…¥å¤±æ•—ï¼Œç¨‹å¼çµæŸ")
        return
    
    # åˆä½µè¨‚å–®è³‡æ–™
    output_path = merge_orders(sales_df, shipping_df, output_dir)
    
    if output_path:
        logging.info("=" * 50)
        logging.info("ğŸ“Š åˆä½µçµæœç¸½çµï¼š")
        logging.info(f"   - ä¸»æª”ï¼ˆéŠ·å”®å ±è¡¨ï¼‰ï¼š{Path(sales_file).name}")
        logging.info(f"   - è¼”æª”ï¼ˆå‡ºè²¨å ±è¡¨ï¼‰ï¼š{Path(shipping_file).name}")
        logging.info(f"   - è¼¸å‡ºæª”æ¡ˆï¼š{Path(output_path).name}")
        logging.info(f"   - è¼¸å‡ºä½ç½®ï¼š{output_path}")
        logging.info("âœ… è¨‚å–®åˆä½µå®Œæˆï¼")
        logging.info("=" * 50)
        
        # æ¸…ç† temp ç›®éŒ„ä¸‹çš„è™•ç†å¾Œæª”æ¡ˆ
        logging.info("é–‹å§‹æ¸…ç† temp ç›®éŒ„ä¸‹çš„è™•ç†å¾Œæª”æ¡ˆ...")
        cleanup_input_files(sales_file, shipping_file)
        
        # æ¸…ç† temp\etmall ç›®éŒ„ä¸‹çš„èˆŠåˆä½µæª”æ¡ˆï¼Œåªä¿ç•™æœ€æ–°çš„
        logging.info("é–‹å§‹æ¸…ç† temp\\etmall ç›®éŒ„ä¸‹çš„èˆŠåˆä½µæª”æ¡ˆ...")
        cleanup_old_merged_files(temp_dir, keep_latest=True)
    else:
        logging.error("âŒ åˆä½µå¤±æ•—ï¼")

if __name__ == "__main__":
    main()
