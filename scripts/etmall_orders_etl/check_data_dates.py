#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è³‡æ–™æ—¥æœŸæª¢æŸ¥è…³æœ¬

ä¸»è¦åŠŸèƒ½ï¼š
- æª¢æŸ¥ data_processed/merged ç›®éŒ„ä¸‹æœ€æ–°çš„ etmall_orders_product_enriched_*.csv æª”æ¡ˆ
- åˆ†æ order_date çš„è³‡æ–™å®Œæ•´æ€§ï¼Œæ‰¾å‡ºéºæ¼çš„æ—¥æœŸ
- æª¢æŸ¥æœ‰ order_date å’Œ order_sn ä½†æ²’æœ‰ product_name_platform çš„è³‡æ–™
- ç”Ÿæˆè©³ç´°çš„æ—¥æœŸåˆ†æå ±å‘Š

Authors: æ¥Šç¿”å¿— & AI Collective
Studio: tranquility-base
"""

import os
import sys
import pandas as pd
import glob
import logging
from datetime import datetime, timedelta
from typing import List, Set, Dict, Any
import openpyxl
from openpyxl.styles import Font, Alignment, PatternFill
from openpyxl.utils.dataframe import dataframe_to_rows

# âœ… å°‡å°ˆæ¡ˆæ ¹ç›®éŒ„åŠ å…¥ sys.path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))

def setup_logging():
    """è¨­å®šæ—¥èªŒç³»çµ±"""
    # å–å¾—è…³æœ¬æ‰€åœ¨ç›®éŒ„
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    # ç¢ºä¿è…³æœ¬ç›®éŒ„å­˜åœ¨
    os.makedirs(script_dir, exist_ok=True)
    
    # åˆªé™¤èˆŠçš„å ±å‘Šæª”æ¡ˆï¼ˆ.txt å’Œ .xlsxï¼‰
    old_txt_files = glob.glob(os.path.join(script_dir, "check_data_dates_*.txt"))
    old_xlsx_files = glob.glob(os.path.join(script_dir, "check_data_dates_*.xlsx"))
    
    for old_file in old_txt_files + old_xlsx_files:
        try:
            os.remove(old_file)
            print(f"[INFO] å·²åˆªé™¤èˆŠå ±å‘Šæª”æ¡ˆï¼š{os.path.basename(old_file)}")
        except Exception as e:
            print(f"[WARN] åˆªé™¤èˆŠå ±å‘Šæª”æ¡ˆå¤±æ•— {os.path.basename(old_file)}: {e}")
    
    # è¨­å®šå ±å‘Šæª”æ¡ˆåç¨±
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = os.path.join(script_dir, f"check_data_dates_{timestamp}.txt")
    excel_file = os.path.join(script_dir, f"check_data_dates_{timestamp}.xlsx")
    
    # è¨­å®šæ—¥èªŒæ ¼å¼
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(report_file, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.excel_file = excel_file  # å°‡ Excel æª”æ¡ˆè·¯å¾‘é™„åŠ åˆ° logger ç‰©ä»¶
    return logger

def get_latest_etmall_file():
    """å–å¾—æœ€æ–°çš„ ETMall ç”¢å“è³‡æ–™è±å¯ŒåŒ– CSV æª”æ¡ˆ"""
    current_dir = os.getcwd()
    
    # å¦‚æœç•¶å‰ç›®éŒ„æ˜¯å°ˆæ¡ˆæ ¹ç›®éŒ„
    if os.path.basename(current_dir) == "ec-data-pipeline":
        pattern = "data_processed/merged/etmall_orders_product_enriched_*.csv"
    else:
        # å¦‚æœå¾å…¶ä»–ç›®éŒ„åŸ·è¡Œï¼Œä½¿ç”¨çµ•å°è·¯å¾‘
        pattern = os.path.join(current_dir, "data_processed/merged/etmall_orders_product_enriched_*.csv")
    
    files = glob.glob(pattern)
    
    if not files:
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°ç¬¦åˆæ¨¡å¼çš„æª”æ¡ˆ: {pattern}")
    
    # æŒ‰ä¿®æ”¹æ™‚é–“æ’åºï¼Œå–å¾—æœ€æ–°çš„æª”æ¡ˆ
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def analyze_date_gaps(df: pd.DataFrame, logger: logging.Logger) -> Dict[str, Any]:
    """åˆ†ææ—¥æœŸé–“éš”ï¼Œæ‰¾å‡ºéºæ¼çš„æ—¥æœŸ"""
    logger.info("=== åˆ†ææ—¥æœŸé–“éš” ===")
    
    # è½‰æ› order_date ç‚º datetime
    df['order_date'] = pd.to_datetime(df['order_date'], errors='coerce')
    
    # ç§»é™¤ç„¡æ•ˆçš„æ—¥æœŸ
    valid_dates = df['order_date'].dropna()
    
    if len(valid_dates) == 0:
        logger.warning("æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ order_date è³‡æ–™")
        return {"missing_dates": [], "date_range": None, "total_days": 0, "missing_count": 0}
    
    # å–å¾—æ—¥æœŸç¯„åœ
    min_date = valid_dates.min()
    max_date = valid_dates.max()
    
    logger.info(f"è³‡æ–™æ—¥æœŸç¯„åœ: {min_date.strftime('%Y-%m-%d')} ~ {max_date.strftime('%Y-%m-%d')}")
    
    # ç”Ÿæˆå®Œæ•´çš„æ—¥æœŸç¯„åœ
    date_range = pd.date_range(start=min_date, end=max_date, freq='D')
    
    # æ‰¾å‡ºå¯¦éš›å­˜åœ¨çš„æ—¥æœŸ
    existing_dates = set(valid_dates.dt.date)
    expected_dates = set(date_range.date)
    
    # æ‰¾å‡ºéºæ¼çš„æ—¥æœŸ
    missing_dates = sorted(expected_dates - existing_dates)
    
    logger.info(f"ç¸½å¤©æ•¸: {len(date_range)}")
    logger.info(f"æœ‰è³‡æ–™çš„å¤©æ•¸: {len(existing_dates)}")
    logger.info(f"éºæ¼çš„å¤©æ•¸: {len(missing_dates)}")
    
    if missing_dates:
        logger.info("éºæ¼çš„æ—¥æœŸ:")
        for date in missing_dates:
            logger.info(f"  - {date}")
    else:
        logger.info("âœ… æ²’æœ‰éºæ¼çš„æ—¥æœŸ")
    
    return {
        "missing_dates": missing_dates,
        "date_range": (min_date, max_date),
        "total_days": len(date_range),
        "missing_count": len(missing_dates),
        "existing_dates": existing_dates
    }

def analyze_missing_shipping_data(df: pd.DataFrame, logger: logging.Logger) -> Dict[str, Any]:
    """åˆ†ææœ‰ order_date å’Œ order_sn ä½†æ²’æœ‰ customer_name çš„è³‡æ–™ï¼ˆç¼ºå°‘å‡ºè²¨è³‡æ–™ï¼‰"""
    logger.info("\n=== åˆ†æç¼ºå°‘å‡ºè²¨è³‡æ–™ ===")
    
    # æª¢æŸ¥æ¬„ä½æ˜¯å¦å­˜åœ¨
    required_columns = ['order_date', 'order_sn', 'customer_name', 'shipping_status']
    missing_columns = [col for col in required_columns if col not in df.columns]
    
    if missing_columns:
        logger.error(f"ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_columns}")
        return {"missing_data": pd.DataFrame(), "date_summary": {}}
    
    logger.info("æª¢æŸ¥ç¼ºå°‘å‡ºè²¨è³‡æ–™ï¼ˆcustomer_name ç‚ºç©ºï¼Œä½†æ’é™¤å·²å–æ¶ˆå’ŒéŠ·é€€çš„è¨‚å–®ï¼‰")
    
    # è½‰æ› order_date ç‚º datetime
    df['order_date'] = pd.to_datetime(df['order_date'], errors='coerce')
    
    # æ‰¾å‡ºæœ‰ order_date å’Œ order_sn ä½†æ²’æœ‰ customer_name çš„è³‡æ–™ï¼ˆç¼ºå°‘å‡ºè²¨è³‡æ–™ï¼‰
    # æ¢ä»¶ï¼šorder_date ä¸ç‚ºç©º AND order_sn ä¸ç‚ºç©º AND (customer_name ç‚ºç©º OR ç‚º NaN) AND shipping_status ä¸åŒ…å«"å–æ¶ˆ"æˆ–"éŠ·é€€"
    missing_shipping_mask = (
        df['order_date'].notna() & 
        df['order_sn'].notna() & 
        (df['customer_name'].isna() | (df['customer_name'] == '') | (df['customer_name'] == 'nan')) &
        (~df['shipping_status'].str.contains('å–æ¶ˆ|éŠ·é€€', na=False))  # æ’é™¤åŒ…å«"å–æ¶ˆ"æˆ–"éŠ·é€€"çš„è¨‚å–®
    )
    
    missing_data = df[missing_shipping_mask].copy()
    
    logger.info(f"ç¼ºå°‘å‡ºè²¨è³‡æ–™çš„ç­†æ•¸: {len(missing_data)}")
    
    if len(missing_data) > 0:
        # æŒ‰æ—¥æœŸåˆ†çµ„çµ±è¨ˆï¼ˆä»¥å»é™¤é‡è¤‡çš„ order_sn ç‚ºå–®ä½ï¼‰
        missing_data['date_str'] = missing_data['order_date'].dt.strftime('%Y-%m-%d')
        date_summary = missing_data.groupby('date_str').agg({
            'order_sn': 'nunique'  # ä½¿ç”¨ nunique ä¾†è¨ˆç®—ä¸é‡è¤‡çš„ order_sn æ•¸é‡
        }).rename(columns={'order_sn': 'unique_order_count'})
        
        logger.info("æŒ‰æ—¥æœŸçµ±è¨ˆç¼ºå°‘å‡ºè²¨è³‡æ–™:")
        for date, row in date_summary.iterrows():
            logger.info(f"  {date}: {row['unique_order_count']} ç­†è¨‚å–®")
        
        # é¡¯ç¤ºæ—¥æœŸç¯„åœ
        if len(missing_data) > 0:
            min_date = missing_data['order_date'].min()
            max_date = missing_data['order_date'].max()
            logger.info(f"ç¼ºå°‘å‡ºè²¨è³‡æ–™çš„æ—¥æœŸç¯„åœ: {min_date.strftime('%Y-%m-%d')} ~ {max_date.strftime('%Y-%m-%d')}")
        
        # é¡¯ç¤ºä¸€äº›ç¯„ä¾‹è³‡æ–™
        logger.info("ç¯„ä¾‹ç¼ºå°‘å‡ºè²¨è³‡æ–™:")
        sample_cols = ['order_date', 'order_sn', 'customer_name', 'shipping_status']
        available_cols = [col for col in sample_cols if col in missing_data.columns]
        
        for idx, row in missing_data[available_cols].head(5).iterrows():
            # è½‰æ› pandas é¡å‹ç‚º Python åŸç”Ÿé¡å‹
            row_dict = {}
            for col in available_cols:
                value = row[col]
                if pd.isna(value):
                    row_dict[col] = None
                elif hasattr(value, 'strftime'):  # datetime é¡å‹
                    row_dict[col] = value.strftime('%Y-%m-%d')
                else:
                    row_dict[col] = str(value)
            logger.info(f"  {row_dict}")
    else:
        logger.info("âœ… æ²’æœ‰ç™¼ç¾ç¼ºå°‘å‡ºè²¨è³‡æ–™")
        date_summary = {}
    
    return {
        "missing_data": missing_data,
        "date_summary": date_summary
    }

def generate_summary_report(date_analysis: Dict[str, Any], shipping_analysis: Dict[str, Any], logger: logging.Logger):
    """ç”Ÿæˆç¸½çµå ±å‘Š"""
    logger.info("\n" + "="*60)
    logger.info("ğŸ“Š è³‡æ–™å®Œæ•´æ€§æª¢æŸ¥ç¸½çµå ±å‘Š")
    logger.info("="*60)
    
    # æ—¥æœŸå®Œæ•´æ€§å ±å‘Š
    logger.info("\nğŸ“… æ—¥æœŸå®Œæ•´æ€§åˆ†æ:")
    if date_analysis["date_range"]:
        min_date, max_date = date_analysis["date_range"]
        logger.info(f"  è³‡æ–™æ—¥æœŸç¯„åœ: {min_date.strftime('%Y-%m-%d')} ~ {max_date.strftime('%Y-%m-%d')}")
        logger.info(f"  ç¸½å¤©æ•¸: {date_analysis['total_days']}")
        logger.info(f"  æœ‰è³‡æ–™çš„å¤©æ•¸: {date_analysis['total_days'] - date_analysis['missing_count']}")
        logger.info(f"  éºæ¼çš„å¤©æ•¸: {date_analysis['missing_count']}")
        
        if date_analysis['missing_count'] > 0:
            logger.info(f"  å®Œæ•´æ€§: {((date_analysis['total_days'] - date_analysis['missing_count']) / date_analysis['total_days'] * 100):.1f}%")
        else:
            logger.info("  å®Œæ•´æ€§: 100%")
    
    # å‡ºè²¨è³‡æ–™å®Œæ•´æ€§å ±å‘Š
    logger.info("\nğŸ“¦ å‡ºè²¨è³‡æ–™å®Œæ•´æ€§åˆ†æ:")
    missing_data = shipping_analysis["missing_data"]
    if len(missing_data) > 0:
        logger.info(f"  ç¼ºå°‘å‡ºè²¨è³‡æ–™çš„è¨˜éŒ„æ•¸: {len(missing_data)}")
        unique_orders = missing_data['order_sn'].nunique()
        logger.info(f"  ç¼ºå°‘å‡ºè²¨è³‡æ–™çš„è¨‚å–®æ•¸: {unique_orders}")
        if not shipping_analysis["date_summary"].empty:
            date_count = len(shipping_analysis["date_summary"])
            logger.info(f"  å½±éŸ¿çš„æ—¥æœŸæ•¸: {date_count}")
            
            # é¡¯ç¤ºå…·é«”çš„æ—¥æœŸåˆ—è¡¨
            logger.info("  ç¼ºå°‘å‡ºè²¨è³‡æ–™çš„æ—¥æœŸ:")
            for date, row in shipping_analysis["date_summary"].iterrows():
                logger.info(f"    - {date}: {row['unique_order_count']} ç­†è¨‚å–®")
    else:
        logger.info("  å‡ºè²¨è³‡æ–™å®Œæ•´æ€§: 100%")
    
    # å»ºè­°
    logger.info("\nğŸ’¡ å»ºè­°:")
    if date_analysis['missing_count'] > 0:
        logger.info("  - æª¢æŸ¥éºæ¼æ—¥æœŸçš„åŸå§‹è³‡æ–™æª”æ¡ˆ")
        logger.info("  - ç¢ºèªè³‡æ–™è™•ç†æµç¨‹æ˜¯å¦å®Œæ•´")
    
    if len(missing_data) > 0:
        logger.info("  - æª¢æŸ¥ä¸Šè¿°æ—¥æœŸçš„å‡ºè²¨è³‡æ–™æ”¶é›†æ˜¯å¦å®Œæ•´")
        logger.info("  - ç¢ºèªå®¢æˆ¶åç¨±æ¬„ä½çš„è³‡æ–™ä¾†æº")
        logger.info("  - è£œé½Šç¼ºå°‘çš„å‡ºè²¨è³‡æ–™")
    
    if date_analysis['missing_count'] == 0 and len(missing_data) == 0:
        logger.info("  - âœ… è³‡æ–™å®Œæ•´æ€§è‰¯å¥½ï¼Œç„¡éœ€ç‰¹åˆ¥è™•ç†")

def generate_excel_report(date_analysis: Dict[str, Any], shipping_analysis: Dict[str, Any], logger: logging.Logger):
    """ç”Ÿæˆ Excel æ ¼å¼çš„ç´”å ±å‘Š"""
    try:
        # å‰µå»ºæ–°çš„å·¥ä½œç°¿
        wb = openpyxl.Workbook()
        
        # ç§»é™¤é è¨­çš„å·¥ä½œè¡¨
        wb.remove(wb.active)
        
        # 1. å»ºç«‹æ‘˜è¦å·¥ä½œè¡¨
        summary_ws = wb.create_sheet("è³‡æ–™å®Œæ•´æ€§æ‘˜è¦")
        
        # è¨­å®šæ¨™é¡Œæ¨£å¼
        title_font = Font(name='å¾®è»Ÿæ­£é»‘é«”', size=14, bold=True, color='FFFFFF')
        title_fill = PatternFill(start_color='366092', end_color='366092', fill_type='solid')
        header_font = Font(name='å¾®è»Ÿæ­£é»‘é«”', size=12, bold=True)
        data_font = Font(name='å¾®è»Ÿæ­£é»‘é«”', size=11)
        
        # æ‘˜è¦æ¨™é¡Œ
        summary_ws['A1'] = 'ETMall è³‡æ–™å®Œæ•´æ€§æª¢æŸ¥å ±å‘Š'
        summary_ws['A1'].font = title_font
        summary_ws['A1'].fill = title_fill
        summary_ws.merge_cells('A1:D1')
        
        # æ—¥æœŸå®Œæ•´æ€§æ‘˜è¦
        row = 3
        summary_ws[f'A{row}'] = 'ğŸ“… æ—¥æœŸå®Œæ•´æ€§åˆ†æ'
        summary_ws[f'A{row}'].font = header_font
        
        if date_analysis["date_range"]:
            min_date, max_date = date_analysis["date_range"]
            summary_ws[f'A{row+1}'] = 'è³‡æ–™æ—¥æœŸç¯„åœ'
            summary_ws[f'B{row+1}'] = f"{min_date.strftime('%Y-%m-%d')} ~ {max_date.strftime('%Y-%m-%d')}"
            summary_ws[f'A{row+2}'] = 'ç¸½å¤©æ•¸'
            summary_ws[f'B{row+2}'] = date_analysis['total_days']
            summary_ws[f'A{row+3}'] = 'æœ‰è³‡æ–™çš„å¤©æ•¸'
            summary_ws[f'B{row+3}'] = date_analysis['total_days'] - date_analysis['missing_count']
            summary_ws[f'A{row+4}'] = 'éºæ¼çš„å¤©æ•¸'
            summary_ws[f'B{row+4}'] = date_analysis['missing_count']
            summary_ws[f'A{row+5}'] = 'å®Œæ•´æ€§'
            if date_analysis['missing_count'] > 0:
                completeness = ((date_analysis['total_days'] - date_analysis['missing_count']) / date_analysis['total_days'] * 100)
                summary_ws[f'B{row+5}'] = f"{completeness:.1f}%"
            else:
                summary_ws[f'B{row+5}'] = "100%"
        
        # å‡ºè²¨è³‡æ–™å®Œæ•´æ€§æ‘˜è¦
        row += 7
        summary_ws[f'A{row}'] = 'ğŸ“¦ å‡ºè²¨è³‡æ–™å®Œæ•´æ€§åˆ†æ'
        summary_ws[f'A{row}'].font = header_font
        
        missing_data = shipping_analysis["missing_data"]
        summary_ws[f'A{row+1}'] = 'ç¼ºå°‘å‡ºè²¨è³‡æ–™çš„è¨˜éŒ„æ•¸'
        summary_ws[f'B{row+1}'] = len(missing_data)
        
        if len(missing_data) > 0:
            # è¨ˆç®—ä¸é‡è¤‡çš„è¨‚å–®æ•¸
            unique_orders = missing_data['order_sn'].nunique()
            summary_ws[f'A{row+2}'] = 'ç¼ºå°‘å‡ºè²¨è³‡æ–™çš„è¨‚å–®æ•¸'
            summary_ws[f'B{row+2}'] = unique_orders
            summary_ws[f'A{row+3}'] = 'å½±éŸ¿çš„æ—¥æœŸæ•¸'
            summary_ws[f'B{row+3}'] = len(shipping_analysis["date_summary"])
        else:
            summary_ws[f'A{row+2}'] = 'å‡ºè²¨è³‡æ–™å®Œæ•´æ€§'
            summary_ws[f'B{row+2}'] = '100%'
        
        # è¨­å®šå­—é«”
        for row in summary_ws.iter_rows():
            for cell in row:
                if cell.value and not cell.font.bold:
                    cell.font = data_font
        
        # 2. å»ºç«‹ç¼ºå°‘å‡ºè²¨è³‡æ–™çš„è©³ç´°å·¥ä½œè¡¨
        if len(missing_data) > 0:
            detail_ws = wb.create_sheet("ç¼ºå°‘å‡ºè²¨è³‡æ–™æ˜ç´°")
            
            # æ¨™é¡Œ
            detail_ws['A1'] = 'ç¼ºå°‘å‡ºè²¨è³‡æ–™çš„æ—¥æœŸæ˜ç´°'
            detail_ws['A1'].font = title_font
            detail_ws['A1'].fill = title_fill
            detail_ws.merge_cells('A1:D1')
            
            # è¡¨é ­
            detail_ws['A3'] = 'æ—¥æœŸ'
            detail_ws['B3'] = 'æ˜ŸæœŸ'
            detail_ws['C3'] = 'ç¼ºå°‘è¨‚å–®æ•¸'
            detail_ws['D3'] = 'å‚™è¨»'
            detail_ws['A3'].font = header_font
            detail_ws['B3'].font = header_font
            detail_ws['C3'].font = header_font
            detail_ws['D3'].font = header_font
            
            # å¡«å…¥è³‡æ–™
            row = 4
            for date, row_data in shipping_analysis["date_summary"].iterrows():
                # è½‰æ›æ—¥æœŸå­—ä¸²ç‚º datetime ç‰©ä»¶ä»¥å–å¾—æ˜ŸæœŸ
                date_obj = pd.to_datetime(date)
                weekday = date_obj.strftime('%A')  # è‹±æ–‡æ˜ŸæœŸ
                weekday_cn = ['æ˜ŸæœŸä¸€', 'æ˜ŸæœŸäºŒ', 'æ˜ŸæœŸä¸‰', 'æ˜ŸæœŸå››', 'æ˜ŸæœŸäº”', 'æ˜ŸæœŸå…­', 'æ˜ŸæœŸæ—¥'][date_obj.weekday()]
                
                detail_ws[f'A{row}'] = date
                detail_ws[f'B{row}'] = weekday_cn
                detail_ws[f'C{row}'] = row_data['unique_order_count']
                detail_ws[f'D{row}'] = 'éœ€è¦è£œé½Šå‡ºè²¨è³‡æ–™'
                detail_ws[f'A{row}'].font = data_font
                detail_ws[f'B{row}'].font = data_font
                detail_ws[f'C{row}'].font = data_font
                detail_ws[f'D{row}'].font = data_font
                row += 1
            
            # è¨­å®šæ¬„å¯¬
            detail_ws.column_dimensions['A'].width = 15
            detail_ws.column_dimensions['B'].width = 10
            detail_ws.column_dimensions['C'].width = 12
            detail_ws.column_dimensions['D'].width = 25
        

        
        # è¨­å®šæ‘˜è¦å·¥ä½œè¡¨çš„æ¬„å¯¬
        summary_ws.column_dimensions['A'].width = 20
        summary_ws.column_dimensions['B'].width = 25
        
        # å„²å­˜æª”æ¡ˆ
        wb.save(logger.excel_file)
        logger.info(f"âœ… Excel å ±å‘Šå·²ç”Ÿæˆï¼š{os.path.basename(logger.excel_file)}")
        
    except Exception as e:
        logger.error(f"âŒ ç”Ÿæˆ Excel å ±å‘Šå¤±æ•—ï¼š{str(e)}")

def main():
    """ä¸»ç¨‹å¼"""
    logger = setup_logging()
    
    try:
        # å–å¾—æœ€æ–°çš„ ETMall æª”æ¡ˆ
        csv_file = get_latest_etmall_file()
        logger.info(f"ä½¿ç”¨æª”æ¡ˆ: {csv_file}")
        
        # è®€å– CSV æª”æ¡ˆ
        logger.info("è®€å– CSV æª”æ¡ˆ...")
        df = pd.read_csv(csv_file, dtype=str, keep_default_na=False)
        logger.info(f"CSV æª”æ¡ˆç­†æ•¸: {len(df)}")
        logger.info(f"CSV æª”æ¡ˆæ¬„ä½æ•¸: {len(df.columns)}")
        
        # é¡¯ç¤ºåŸºæœ¬è³‡è¨Š
        logger.info("\n=== æª”æ¡ˆåŸºæœ¬è³‡è¨Š ===")
        logger.info(f"æª”æ¡ˆè·¯å¾‘: {csv_file}")
        logger.info(f"æª”æ¡ˆå¤§å°: {os.path.getsize(csv_file) / 1024 / 1024:.2f} MB")
        logger.info(f"æœ€å¾Œä¿®æ”¹æ™‚é–“: {datetime.fromtimestamp(os.path.getmtime(csv_file)).strftime('%Y-%m-%d %H:%M:%S')}")
        
        # åˆ†ææ—¥æœŸé–“éš”
        date_analysis = analyze_date_gaps(df, logger)
        
        # åˆ†æç¼ºå°‘å‡ºè²¨è³‡æ–™
        shipping_analysis = analyze_missing_shipping_data(df, logger)
        
        # ç”Ÿæˆç¸½çµå ±å‘Š
        generate_summary_report(date_analysis, shipping_analysis, logger)
        
        # ç”Ÿæˆ Excel å ±å‘Š
        generate_excel_report(date_analysis, shipping_analysis, logger)
        
        logger.info("\nâœ… è³‡æ–™æ—¥æœŸæª¢æŸ¥å®Œæˆ")
        
    except Exception as e:
        logger.error(f"âŒ åŸ·è¡Œå¤±æ•—: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()
