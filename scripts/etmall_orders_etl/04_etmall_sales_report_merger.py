#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ±æ£®è³¼ç‰©éŠ·å”®å ±è¡¨åˆä½µè…³æœ¬
åˆä½µ temp/etmall/Sales_Report ä¸‹æ‰€æœ‰ CSV æª”æ¡ˆï¼Œä¸¦ä»¥ order_line_uid ç‚º key å»é™¤é‡è¤‡
"""

import os
import sys
import pandas as pd
import logging
from pathlib import Path
from datetime import datetime

# è¨­å®šå°ˆæ¡ˆæ ¹ç›®éŒ„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(project_root / 'logs' / 'etmall_sales_report_merger.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

def find_sales_report_files(sales_report_dir: Path) -> list:
    """
    å°‹æ‰¾æ‰€æœ‰éŠ·å”®å ±è¡¨ CSV æª”æ¡ˆ
    
    Args:
        sales_report_dir: éŠ·å”®å ±è¡¨ç›®éŒ„
        
    Returns:
        list: CSV æª”æ¡ˆè·¯å¾‘åˆ—è¡¨
    """
    csv_files = []
    
    if not sales_report_dir.exists():
        logging.warning(f"éŠ·å”®å ±è¡¨ç›®éŒ„ä¸å­˜åœ¨ï¼š{sales_report_dir}")
        return csv_files
    
    # éæ­¸æœå°‹æ‰€æœ‰ CSV æª”æ¡ˆ
    for file_path in sales_report_dir.rglob("*.csv"):
        if file_path.is_file():
            csv_files.append(file_path)
            logging.info(f"æ‰¾åˆ°éŠ·å”®å ±è¡¨æª”æ¡ˆï¼š{file_path}")
    
    return sorted(csv_files)

def merge_sales_report_files(csv_files: list, output_dir: Path) -> str:
    """
    åˆä½µéŠ·å”®å ±è¡¨æª”æ¡ˆä¸¦å»é™¤é‡è¤‡
    
    Args:
        csv_files: CSV æª”æ¡ˆè·¯å¾‘åˆ—è¡¨
        output_dir: è¼¸å‡ºç›®éŒ„
        
    Returns:
        str: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
    """
    if not csv_files:
        logging.warning("æ²’æœ‰æ‰¾åˆ°ä»»ä½• CSV æª”æ¡ˆ")
        return None
    
    # å»ºç«‹è¼¸å‡ºç›®éŒ„
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆä½µæ‰€æœ‰ CSV æª”æ¡ˆ
    all_dataframes = []
    
    for file_path in csv_files:
        try:
            logging.info(f"è®€å–æª”æ¡ˆï¼š{file_path.name}")
            
            # è®€å– CSV æª”æ¡ˆ
            df = pd.read_csv(file_path, encoding='utf-8-sig', dtype=str)
            
            # è¨˜éŒ„æª”æ¡ˆè³‡è¨Š
            logging.info(f"  - æª”æ¡ˆï¼š{file_path.name}")
            logging.info(f"  - è³‡æ–™ç­†æ•¸ï¼š{len(df)} ç­†")
            logging.info(f"  - æ¬„ä½æ•¸é‡ï¼š{len(df.columns)} å€‹")
            
            # æª¢æŸ¥æ˜¯å¦æœ‰ order_line_uid æ¬„ä½
            if 'order_line_uid' not in df.columns:
                logging.warning(f"æª”æ¡ˆ {file_path.name} ç¼ºå°‘ order_line_uid æ¬„ä½ï¼Œè·³é")
                continue
            
            all_dataframes.append(df)
            
        except Exception as e:
            logging.error(f"è®€å–æª”æ¡ˆå¤±æ•—ï¼š{file_path.name} - {str(e)}")
            continue
    
    if not all_dataframes:
        logging.error("æ²’æœ‰æˆåŠŸè®€å–ä»»ä½•æª”æ¡ˆ")
        return None
    
    # åˆä½µæ‰€æœ‰ DataFrame
    logging.info("é–‹å§‹åˆä½µæ‰€æœ‰è³‡æ–™...")
    merged_df = pd.concat(all_dataframes, ignore_index=True)
    
    logging.info(f"åˆä½µå‰ç¸½è³‡æ–™ç­†æ•¸ï¼š{len(merged_df)} ç­†")
    
    # æª¢æŸ¥é‡è¤‡çš„ order_line_uid
    duplicate_count = merged_df['order_line_uid'].duplicated().sum()
    logging.info(f"ç™¼ç¾é‡è¤‡çš„ order_line_uidï¼š{duplicate_count} ç­†")
    
    if duplicate_count > 0:
        # é¡¯ç¤ºé‡è¤‡çš„ order_line_uid ç¯„ä¾‹
        duplicates = merged_df[merged_df['order_line_uid'].duplicated(keep=False)]['order_line_uid'].unique()[:10]
        logging.info(f"é‡è¤‡çš„ order_line_uid ç¯„ä¾‹ï¼š{list(duplicates)}")
        
        # å»é™¤é‡è¤‡ï¼Œä¿ç•™æœ€å¾Œä¸€ç­†ï¼ˆæ–°è“‹èˆŠï¼‰
        merged_df = merged_df.drop_duplicates(subset=['order_line_uid'], keep='last')
        logging.info(f"å»é™¤é‡è¤‡å¾Œè³‡æ–™ç­†æ•¸ï¼š{len(merged_df)} ç­†")
        logging.info(f"ç§»é™¤é‡è¤‡è³‡æ–™ï¼š{duplicate_count} ç­†")
    
    # è™•ç† item_no æ ¼å¼ï¼ˆå€‹ä½æ•¸å‰é¢è£œ0ï¼‰
    if 'item_no' in merged_df.columns:
        merged_df['item_no'] = merged_df['item_no'].astype(str).str.zfill(2)
        logging.info("å·²è™•ç† item_no æ ¼å¼ï¼ˆå€‹ä½æ•¸å‰é¢è£œ0ï¼‰")
    
    # æŒ‰ç…§ order_sn å’Œ item_no æ’åº
    if 'order_sn' in merged_df.columns and 'item_no' in merged_df.columns:
        # è½‰æ›ç‚ºæ•¸å€¼å‹æ…‹é€²è¡Œæ’åº
        merged_df['order_sn_numeric'] = pd.to_numeric(merged_df['order_sn'], errors='coerce')
        merged_df['item_no_numeric'] = pd.to_numeric(merged_df['item_no'], errors='coerce')
        
        # æ’åº
        merged_df = merged_df.sort_values(['order_sn_numeric', 'item_no_numeric'], ascending=[True, True])
        
        # ç§»é™¤è‡¨æ™‚æ¬„ä½
        merged_df = merged_df.drop(['order_sn_numeric', 'item_no_numeric'], axis=1)
        
        logging.info("å·²æŒ‰ç…§ order_sn å’Œ item_no æ’åºï¼ˆç”±å°åˆ°å¤§ï¼‰")
    
    # ç”Ÿæˆè¼¸å‡ºæª”æ¡ˆåç¨±
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_filename = f"etmall_sales_report_merged_{timestamp}.csv"
    output_path = output_dir / output_filename
    
    # å„²å­˜åˆä½µå¾Œçš„è³‡æ–™
    try:
        merged_df.to_csv(output_path, index=False, encoding='utf-8-sig')
        logging.info(f"âœ… åˆä½µå®Œæˆï¼š{output_filename}")
        logging.info(f"   è¼¸å‡ºä½ç½®ï¼š{output_path}")
        logging.info(f"   æœ€çµ‚è³‡æ–™ç­†æ•¸ï¼š{len(merged_df)} ç­†")
        logging.info(f"   æ¬„ä½æ•¸é‡ï¼š{len(merged_df.columns)} å€‹")
        
        return str(output_path)
        
    except Exception as e:
        logging.error(f"å„²å­˜æª”æ¡ˆå¤±æ•—ï¼š{str(e)}")
        return None

def cleanup_input_files(csv_files: list):
    """
    æ¸…ç†è¼¸å…¥æª”æ¡ˆ
    
    Args:
        csv_files: è¦åˆªé™¤çš„ CSV æª”æ¡ˆè·¯å¾‘åˆ—è¡¨
    """
    deleted_count = 0
    for file_path in csv_files:
        try:
            if file_path.exists():
                file_path.unlink()
                logging.info(f"å·²åˆªé™¤è¼¸å…¥æª”æ¡ˆï¼š{file_path.name}")
                deleted_count += 1
        except Exception as e:
            logging.warning(f"åˆªé™¤æª”æ¡ˆå¤±æ•—ï¼š{file_path.name} - {str(e)}")
    
    logging.info(f"ç¸½å…±åˆªé™¤ {deleted_count} å€‹è¼¸å…¥æª”æ¡ˆ")

def cleanup_old_merged_files(output_dir: Path, keep_latest: bool = True):
    """
    æ¸…ç† temp\\etmall ç›®éŒ„ä¸‹çš„èˆŠéŠ·å”®å ±è¡¨åˆä½µæª”æ¡ˆï¼Œåªä¿ç•™æœ€æ–°çš„
    
    Args:
        output_dir: temp\\etmall ç›®éŒ„è·¯å¾‘
        keep_latest: æ˜¯å¦ä¿ç•™æœ€æ–°çš„æª”æ¡ˆ
    """
    if not output_dir.exists():
        logging.warning(f"ç›®éŒ„ä¸å­˜åœ¨ï¼š{output_dir}")
        return
    
    # å°‹æ‰¾æ‰€æœ‰éŠ·å”®å ±è¡¨åˆä½µæª”æ¡ˆ
    merged_files = list(output_dir.glob("etmall_sales_report_merged_*.csv"))
    
    if not merged_files:
        logging.info("æ²’æœ‰æ‰¾åˆ°éœ€è¦æ¸…ç†çš„éŠ·å”®å ±è¡¨åˆä½µæª”æ¡ˆ")
        return
    
    # æŒ‰ä¿®æ”¹æ™‚é–“æ’åºï¼Œæœ€æ–°çš„åœ¨æœ€å¾Œ
    merged_files.sort(key=lambda x: x.stat().st_mtime)
    
    if keep_latest:
        # ä¿ç•™æœ€æ–°çš„æª”æ¡ˆ
        files_to_delete = merged_files[:-1]  # é™¤äº†æœ€å¾Œä¸€å€‹ï¼ˆæœ€æ–°çš„ï¼‰
        files_to_keep = merged_files[-1:]    # åªä¿ç•™æœ€æ–°çš„
    else:
        # åˆªé™¤æ‰€æœ‰æª”æ¡ˆ
        files_to_delete = merged_files
        files_to_keep = []
    
    deleted_count = 0
    for file_path in files_to_delete:
        try:
            if file_path.exists():
                file_path.unlink()
                logging.info(f"å·²åˆªé™¤èˆŠæª”æ¡ˆï¼š{file_path.name}")
                deleted_count += 1
        except Exception as e:
            logging.warning(f"åˆªé™¤æª”æ¡ˆå¤±æ•—ï¼š{file_path.name} - {str(e)}")
    
    if files_to_keep:
        logging.info(f"ä¿ç•™æœ€æ–°æª”æ¡ˆï¼š{files_to_keep[0].name}")
    
    logging.info(f"ç¸½å…±åˆªé™¤ {deleted_count} å€‹èˆŠéŠ·å”®å ±è¡¨åˆä½µæª”æ¡ˆ")

def main():
    """ä¸»å‡½æ•¸"""
    logging.info("=" * 50)
    logging.info("é–‹å§‹åŸ·è¡Œæ±æ£®è³¼ç‰©éŠ·å”®å ±è¡¨åˆä½µè…³æœ¬")
    logging.info("=" * 50)
    
    # è¨­å®šè·¯å¾‘
    sales_report_dir = project_root / "temp" / "etmall" / "Sales_Report"
    output_dir = project_root / "temp" / "etmall"
    
    logging.info(f"å°ˆæ¡ˆæ ¹ç›®éŒ„ï¼š{project_root}")
    logging.info(f"éŠ·å”®å ±è¡¨ç›®éŒ„ï¼š{sales_report_dir}")
    logging.info(f"è¼¸å‡ºç›®éŒ„ï¼š{output_dir}")
    
    # å°‹æ‰¾æ‰€æœ‰ CSV æª”æ¡ˆ
    csv_files = find_sales_report_files(sales_report_dir)
    
    if not csv_files:
        logging.error("æ²’æœ‰æ‰¾åˆ°ä»»ä½• CSV æª”æ¡ˆï¼Œç¨‹å¼çµæŸ")
        return
    
    logging.info(f"æ‰¾åˆ° {len(csv_files)} å€‹ CSV æª”æ¡ˆéœ€è¦åˆä½µ")
    
    # åˆä½µæª”æ¡ˆ
    output_path = merge_sales_report_files(csv_files, output_dir)
    
    if output_path:
        logging.info("=" * 50)
        logging.info("ğŸ“Š åˆä½µçµæœç¸½çµï¼š")
        logging.info(f"   - è™•ç†æª”æ¡ˆï¼š{len(csv_files)} å€‹")
        logging.info(f"   - è¼¸å‡ºæª”æ¡ˆï¼š{Path(output_path).name}")
        logging.info(f"   - è¼¸å‡ºä½ç½®ï¼š{output_path}")
        logging.info("âœ… éŠ·å”®å ±è¡¨åˆä½µå®Œæˆï¼")
        logging.info("=" * 50)
        
        # æ¸…ç† temp ç›®éŒ„ä¸‹çš„è™•ç†å¾Œæª”æ¡ˆ
        logging.info("é–‹å§‹æ¸…ç† temp ç›®éŒ„ä¸‹çš„è™•ç†å¾Œæª”æ¡ˆ...")
        cleanup_input_files(csv_files)
        
        # æ¸…ç† temp\\etmall ç›®éŒ„ä¸‹çš„èˆŠéŠ·å”®å ±è¡¨åˆä½µæª”æ¡ˆï¼Œåªä¿ç•™æœ€æ–°çš„
        logging.info("é–‹å§‹æ¸…ç† temp\\etmall ç›®éŒ„ä¸‹çš„èˆŠéŠ·å”®å ±è¡¨åˆä½µæª”æ¡ˆ...")
        cleanup_old_merged_files(output_dir, keep_latest=True)
    else:
        logging.error("âŒ åˆä½µå¤±æ•—ï¼")

if __name__ == "__main__":
    main()
