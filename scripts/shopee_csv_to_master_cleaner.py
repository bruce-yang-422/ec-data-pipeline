"""
shopee_csv_to_master_cleaner.py

åŠŸèƒ½ï¼š
- æ‰¹æ¬¡è®€å– temp/shopee/*.csv æª”æ¡ˆ
- æŒ‰ shopee_fields_mapping.json å®šç¾©èª¿æ•´æ¬„ä½
- è¼¸å‡ºåˆ° shopee_master_orders_cleaned.csv

ä½¿ç”¨ï¼špython shopee_csv_to_master_cleaner.py

è¼¸å…¥ï¼š
- temp/shopee/*.csv æª”æ¡ˆ
- config/shopee_fields_mapping.json

è¼¸å‡ºï¼š
- data_processed/merged/shopee_master_orders_cleaned.csv

Authors: æ¥Šç¿”å¿— & AI Collective
Studio: tranquility-base
"""

import pandas as pd
import json
from datetime import datetime
import os
from glob import glob
import re

MAPPING_PATH = "config/shopee_fields_mapping.json"
SOURCE_DIR = "temp/shopee"
OUTPUT_PATH = "data_processed/merged/shopee_master_orders_cleaned.csv"

def get_mapping():
    """è®€å– shopee mapping è¨­å®šï¼ŒæŒ‰ç…§ order æ’åºï¼Œä¸¦å»ºç«‹ä¸­è‹±æ–‡å°æ‡‰"""
    try:
        with open(MAPPING_PATH, "r", encoding="utf-8") as f:
            mapping = json.load(f)
    except FileNotFoundError:
        print(f"è­¦å‘Š: æ‰¾ä¸åˆ° mapping æª”æ¡ˆ {MAPPING_PATH}")
        return {}, [], {}, {}
    except json.JSONDecodeError as e:
        print(f"è­¦å‘Š: mapping æª”æ¡ˆæ ¼å¼éŒ¯èª¤ {MAPPING_PATH}: {e}")
        return {}, [], {}, {}
    
    # æŒ‰ç…§ order æ¬„ä½æ’åºï¼Œç¢ºä¿æ¬„ä½é †åºæ­£ç¢º
    columns = sorted(mapping.keys(), key=lambda k: int(mapping[k].get("order", "999")))
    
    # å»ºç«‹ä¸­è‹±æ–‡å°æ‡‰ï¼ˆä¸­æ–‡ -> è‹±æ–‡ï¼‰
    zh2en = {v.get("zh_name", k): k for k, v in mapping.items() if v.get("zh_name")}
    
    # å»ºç«‹è‹±ä¸­æ–‡å°æ‡‰ï¼ˆè‹±æ–‡ -> ä¸­æ–‡ï¼‰
    en2zh = {k: v.get("zh_name", k) for k, v in mapping.items()}
    
    print(f"è¼‰å…¥ mapping: {len(columns)} å€‹æ¬„ä½")
    print(f"ä¸­è‹±å°æ‡‰: {len(zh2en)} å€‹å°æ‡‰é—œä¿‚")
    
    return mapping, columns, en2zh, zh2en

def parse_shop_info(filename, df):
    """è§£æåº—é‹ªè³‡è¨Š"""
    # å„ªå…ˆå¾æª”æ¡ˆå…§å®¹è®€å–
    if 'shop_name' in df.columns and 'shop_account' in df.columns:
        shop_name = df['shop_name'].dropna().iloc[0] if len(df['shop_name'].dropna()) > 0 else ""
        shop_account = df['shop_account'].dropna().iloc[0] if len(df['shop_account'].dropna()) > 0 else ""
        if shop_name and shop_account:
            return str(shop_name), str(shop_account)
    
    # å¾æª”åè§£æ
    base = os.path.basename(filename)
    m = re.match(r"(.+?)_([\w\d]+)_Order", base)
    if m:
        return m.group(1), m.group(2)
    
    return "unknown_shop", "unknown_account"

def read_csv_files(zh2en_mapping):
    """è®€å–æ‰€æœ‰ CSV æª”æ¡ˆä¸¦ä½¿ç”¨ mapping æª”æ¡ˆé€²è¡Œæ¬„ä½åç¨±å°æ‡‰"""
    csv_files = glob(os.path.join(SOURCE_DIR, "*.csv"))
    if not csv_files:
        return pd.DataFrame()
    
    dfs = []
    for file in csv_files:
        try:
            df = pd.read_csv(file, dtype=str, encoding='utf-8-sig').fillna("")
            
            print(f"\nè™•ç†æª”æ¡ˆ: {os.path.basename(file)}")
            print(f"åŸå§‹æ¬„ä½æ•¸: {len(df.columns)}")
            print(f"åŸå§‹æ¬„ä½ç¯„ä¾‹: {list(df.columns)[:5]}...")
            
            # ä½¿ç”¨ mapping æª”æ¡ˆé€²è¡Œæ¬„ä½åç¨±å°æ‡‰
            original_columns = df.columns.tolist()
            mapped_columns = {}
            unmapped_columns = []
            
            for col in original_columns:
                # æ¸…ç†æ¬„ä½åç¨±ï¼šç§»é™¤æ›è¡Œç¬¦è™Ÿã€å¤šé¤˜ç©ºç™½ã€æ¨™é»ç¬¦è™Ÿå·®ç•°
                col_cleaned = col.strip().replace('\n', '').replace('\r', '').replace('ï¼š', ':').replace('ï¼ˆ', '(').replace('ï¼‰', ')')
                col_cleaned = ' '.join(col_cleaned.split())  # ç§»é™¤å¤šé¤˜ç©ºç™½
                
                found_match = False
                
                # 1. ç›´æ¥å°æ‡‰ï¼ˆåŸæ¬„ä½åï¼‰
                if col in zh2en_mapping:
                    mapped_columns[col] = zh2en_mapping[col]
                    found_match = True
                
                # 2. æ¸…ç†å¾Œç›´æ¥å°æ‡‰
                elif col_cleaned in zh2en_mapping:
                    mapped_columns[col] = zh2en_mapping[col_cleaned]
                    found_match = True
                
                # 3. æ¨¡ç³ŠåŒ¹é…
                else:
                    for zh_name, en_name in zh2en_mapping.items():
                        zh_name_cleaned = zh_name.strip().replace('\n', '').replace('\r', '').replace('ï¼š', ':').replace('ï¼ˆ', '(').replace('ï¼‰', ')')
                        zh_name_cleaned = ' '.join(zh_name_cleaned.split())  # ç§»é™¤å¤šé¤˜ç©ºç™½
                        
                        # å®Œå…¨åŒ¹é…ï¼ˆæ¸…ç†å¾Œï¼‰
                        if col_cleaned == zh_name_cleaned:
                            mapped_columns[col] = en_name
                            found_match = True
                            break
                        
                        # åŒ…å«é—œä¿‚åŒ¹é…ï¼ˆé¿å…éçŸ­çš„èª¤åŒ¹é…ï¼‰
                        elif len(col_cleaned) > 5 and len(zh_name_cleaned) > 5:
                            if col_cleaned in zh_name_cleaned or zh_name_cleaned in col_cleaned:
                                # æª¢æŸ¥ç›¸ä¼¼åº¦ï¼Œé¿å…èª¤åŒ¹é…
                                shorter = min(len(col_cleaned), len(zh_name_cleaned))
                                longer = max(len(col_cleaned), len(zh_name_cleaned))
                                if shorter / longer > 0.7:  # ç›¸ä¼¼åº¦è¶…é70%
                                    mapped_columns[col] = en_name
                                    found_match = True
                                    break
                
                if not found_match:
                    unmapped_columns.append(col)
                    # é¡¯ç¤ºæ¸…ç†å¾Œçš„æ¬„ä½åï¼Œæ–¹ä¾¿é™¤éŒ¯
                    print(f"  ç„¡æ³•å°æ‡‰: '{col}' -> æ¸…ç†å¾Œ: '{col_cleaned}'")
            
            print(f"æˆåŠŸå°æ‡‰: {len(mapped_columns)} å€‹æ¬„ä½")
            print(f"ç„¡æ³•å°æ‡‰: {len(unmapped_columns)} å€‹æ¬„ä½")
            
            # é¡¯ç¤ºæˆåŠŸå°æ‡‰çš„é—œéµæ¬„ä½
            key_mappings = {k: v for k, v in mapped_columns.items() if v in ['order_sn', 'product_name', 'product_sku_main', 'buyer_username']}
            if key_mappings:
                print(f"é—œéµæ¬„ä½å°æ‡‰: {key_mappings}")
            
            if unmapped_columns and len(unmapped_columns) <= 5:
                print(f"æœªå°æ‡‰æ¬„ä½è©³ç´°: {unmapped_columns}")  # å°‘æ–¼5å€‹å°±å…¨éƒ¨é¡¯ç¤º
            
            # æ‡‰ç”¨æ¬„ä½åç¨±å°æ‡‰
            df = df.rename(columns=mapped_columns)
            
            # éæ¿¾ç©ºè¡Œï¼ˆä¸»è¦æ¬„ä½éƒ½ç‚ºç©ºçš„è¡Œï¼‰
            if 'order_sn' in df.columns:
                before_filter = len(df)
                df = df[df['order_sn'].str.strip() != ""]
                print(f"éæ¿¾ç©ºè¨‚å–®: {before_filter} -> {len(df)} ç­†")
            elif 'è¨‚å–®ç·¨è™Ÿ' in df.columns:
                # å¦‚æœé‚„æ˜¯ä¸­æ–‡æ¬„ä½åï¼Œä¹Ÿè¦è™•ç†
                before_filter = len(df)
                df = df[df['è¨‚å–®ç·¨è™Ÿ'].str.strip() != ""]
                df = df.rename(columns={'è¨‚å–®ç·¨è™Ÿ': 'order_sn'})
                print(f"éæ¿¾ç©ºè¨‚å–®(ä¸­æ–‡æ¬„ä½): {before_filter} -> {len(df)} ç­†")
            
            # æ¸…ç†æ›è¡Œç¬¦è™Ÿå’Œæ•¸å­—æ ¼å¼
            for col in df.columns:
                if df[col].dtype == 'object':
                    df[col] = df[col].astype(str).str.replace('\n', ' ').str.replace('\r', ' ').str.strip()
                    # ç§»é™¤æ•¸å­—æ¬„ä½çš„ .0 å¾Œç¶´
                    if col in ['product_sku_main', 'product_sku_variation', 'quantity', 'return_quantity']:
                        df[col] = df[col].str.replace(r'\.0$', '', regex=True)
            
            # è§£æåº—é‹ªè³‡è¨Š
            shop_name, shop_account = parse_shop_info(file, df)
            df["shop_name"] = shop_name
            df["shop_account"] = shop_account
            
            # è™•ç† order_date - å¾ order_creation_timestamp æˆ– order_sn è§£æ
            if 'order_creation_timestamp' in df.columns and 'order_date' not in df.columns:
                # å¾å®Œæ•´æ™‚é–“æˆ³è§£ææ—¥æœŸ
                df['order_date'] = pd.to_datetime(df['order_creation_timestamp'], errors='coerce').dt.strftime('%Y-%m-%d')
                df['order_date'] = df['order_date'].fillna('')
                print(f"å¾ order_creation_timestamp è§£æ order_date")
            elif 'order_sn' in df.columns and 'order_date' not in df.columns:
                # å¾ order_sn å‰6ç¢¼è§£ææ—¥æœŸ (YYMMDD -> YYYY-MM-DD)
                def parse_date_from_sn(sn):
                    try:
                        if len(str(sn)) >= 6:
                            date_part = str(sn)[:6]
                            year = int('20' + date_part[:2])
                            month = int(date_part[2:4])
                            day = int(date_part[4:6])
                            return f"{year:04d}-{month:02d}-{day:02d}"
                    except:
                        pass
                    return ''
                
                df['order_date'] = df['order_sn'].apply(parse_date_from_sn)
                print(f"å¾ order_sn è§£æ order_date")
            
            print(f"æœ€çµ‚æ¬„ä½æ•¸: {len(df.columns)}")
            print(f"æœ€çµ‚æ¬„ä½ç¯„ä¾‹: {[col for col in df.columns if col in ['order_sn', 'order_date', 'product_name', 'shop_name']]}")
            
            dfs.append(df)
            print(f"âœ… è®€å–æˆåŠŸ: {len(df)} ç­†è³‡æ–™")
            
        except Exception as e:
            print(f"âŒ è®€å–å¤±æ•—: {file} - {e}")
    
    combined_df = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()
    print(f"\nğŸ“Š ç¸½è¨ˆè®€å–: {len(combined_df)} ç­†è³‡æ–™")
    return combined_df

def process_data(df, mapping, columns):
    """è™•ç†è³‡æ–™ï¼Œå®Œå…¨åƒç…§ shopee_fields_mapping.json"""
    
    print(f"é–‹å§‹è™•ç†è³‡æ–™: {len(df)} ç­†")
    
    # åŸºæœ¬æ¬„ä½è¨­å®šï¼ˆåƒç…§ mapping æª”æ¡ˆé †åºï¼‰
    df['platform'] = 'shopee'  # order: 1, å›ºå®šå€¼
    df['processing_date'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')  # order: 4
    
    # ç”¢ç”Ÿ item_seq (order: 7)
    if 'order_date' in df.columns and 'order_sn' in df.columns:
        df = df.sort_values(['order_date', 'order_sn']).reset_index(drop=True)
        df['item_seq'] = df.groupby(['order_date', 'order_sn']).cumcount() + 1
        print(f"ç”¢ç”Ÿ item_seq å®Œæˆ")
    else:
        df['item_seq'] = 1
        print(f"ä½¿ç”¨é è¨­ item_seq = 1")
    
    # æª¢æŸ¥é—œéµæ¬„ä½
    print(f"order_date æ¬„ä½å­˜åœ¨: {'order_date' in df.columns}")
    print(f"order_sn æ¬„ä½å­˜åœ¨: {'order_sn' in df.columns}")
    print(f"product_sku_main æ¬„ä½å­˜åœ¨: {'product_sku_main' in df.columns}")
    
    if 'order_date' in df.columns:
        empty_order_date = len(df[df['order_date'].fillna('').astype(str).str.strip() == ''])
        print(f"ç©ºçš„ order_date: {empty_order_date} ç­†")
    
    if 'order_sn' in df.columns:
        empty_order_sn = len(df[df['order_sn'].fillna('').astype(str).str.strip() == ''])
        print(f"ç©ºçš„ order_sn: {empty_order_sn} ç­†")
    
    # å»ºç«‹ key_for_merge (order: 60) - æŒ‰ç…§ mapping å®šç¾©
    # order_sn + product_sku_main + product_sku_variation
    order_sn_part = df['order_sn'].fillna('').astype(str) if 'order_sn' in df.columns else pd.Series([''] * len(df))
    sku_main_part = df['product_sku_main'].fillna('').astype(str) if 'product_sku_main' in df.columns else pd.Series([''] * len(df))
    sku_variation_part = df['product_sku_variation'].fillna('').astype(str) if 'product_sku_variation' in df.columns else pd.Series([''] * len(df))
    
    df['key_for_merge'] = order_sn_part + '_' + sku_main_part + '_' + sku_variation_part
    df['duplicate_key'] = df['key_for_merge']  # ç”¨æ–¼å»é‡
    
    print(f"å»ºç«‹ key_for_merge å®Œæˆï¼Œç¯„ä¾‹: {df['key_for_merge'].iloc[0] if len(df) > 0 else 'N/A'}")
    
    # ä¿®æ­£ï¼šä¸€æ¬¡æ€§æ·»åŠ ç¼ºå¤±çš„æ¬„ä½ï¼Œé¿å…ç¢ç‰‡åŒ–
    missing_cols = [col for col in columns if col not in df.columns]
    print(f"ç¼ºå¤±æ¬„ä½: {len(missing_cols)} å€‹")
    
    if missing_cols:
        # æ ¹æ“š mapping è¨­å®šé è¨­å€¼
        missing_data = {}
        for col in missing_cols:
            if col in mapping:
                col_type = mapping[col].get('type', 'STRING')
                if col_type in ['NUMERIC', 'INTEGER']:
                    missing_data[col] = 0
                elif col_type in ['DATE', 'TIMESTAMP']:
                    missing_data[col] = ''
                else:  # STRING
                    missing_data[col] = ''
            else:
                missing_data[col] = ''
        
        # å»ºç«‹ç¼ºå¤±æ¬„ä½çš„ DataFrame
        missing_df = pd.DataFrame(missing_data, index=df.index)
        # ä¸€æ¬¡æ€§åˆä½µæ‰€æœ‰ç¼ºå¤±æ¬„ä½
        df = pd.concat([df, missing_df], axis=1)
        print(f"æ·»åŠ ç¼ºå¤±æ¬„ä½å®Œæˆ")
    
    # æ ¹æ“š mapping æª”æ¡ˆè¨­å®šè³‡æ–™é¡å‹
    for col in columns:
        if col in mapping:
            col_type = mapping[col].get('type', 'STRING')
            
            # æ•¸å€¼é¡å‹è™•ç†
            if col_type == 'NUMERIC':
                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0.0)
            elif col_type == 'INTEGER':
                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype('Int64')  # æ”¯æ´ null çš„æ•´æ•¸
            
            # æ—¥æœŸæ™‚é–“è™•ç†
            elif col_type in ['DATE', 'TIMESTAMP']:
                # ä¿æŒå­—ä¸²æ ¼å¼ï¼Œè®“å¾ŒçºŒè™•ç†æ±ºå®šæ—¥æœŸæ ¼å¼
                df[col] = df[col].astype(str).replace('nan', '').replace('0', '').replace('0.0', '')
            
            # å­—ä¸²é¡å‹è™•ç†
            else:  # STRING
                df[col] = df[col].astype(str).replace('nan', '').replace('None', '')
    
    print(f"è³‡æ–™é¡å‹è½‰æ›å®Œæˆ")
    
    # æŒ‰ç…§ mapping æª”æ¡ˆçš„ order é †åºé‡æ–°æ’åˆ—æ¬„ä½
    ordered_columns = sorted([col for col in columns if col in df.columns], 
                           key=lambda x: int(mapping.get(x, {}).get('order', '999')))
    
    # ç¢ºä¿é—œéµæ¬„ä½åŒ…å«åœ¨çµæœä¸­
    result_columns = ordered_columns + ['duplicate_key']
    
    # æ·»åŠ é¡å¤–çš„ç³»çµ±æ¬„ä½ï¼ˆä¸åœ¨ mapping ä¸­ä½†éœ€è¦çš„ï¼‰
    extra_columns = ['key_for_merge']  # ç§»é™¤ data_import_timestampï¼Œé€™å€‹åœ¨ save_data ä¸­è™•ç†
    for col in extra_columns:
        if col in df.columns and col not in result_columns:
            result_columns.append(col)
    
    print(f"æœ€çµ‚æ¬„ä½æ•¸: {len(result_columns)} å€‹")
    
    return df[result_columns]

def save_data(df, mapping):
    """å„²å­˜è³‡æ–™ï¼Œæ™ºæ…§æ–°è³‡æ–™è¦†è“‹èˆŠè³‡æ–™"""
    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)
    
    print(f"æº–å‚™å„²å­˜è³‡æ–™: {len(df)} ç­†")
    
    # ç‚ºæ–°è³‡æ–™æ·»åŠ åŒ¯å…¥æ™‚é–“æˆ³
    current_timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    df['data_import_timestamp'] = current_timestamp
    
    # åˆä½µç¾æœ‰è³‡æ–™
    if os.path.exists(OUTPUT_PATH):
        try:
            old_df = pd.read_csv(OUTPUT_PATH, dtype=str).fillna("")
            print(f"è®€å–åˆ°èˆŠè³‡æ–™: {len(old_df)} ç­†")
            
            # ç¢ºä¿èˆŠè³‡æ–™ä¹Ÿæœ‰åŒ¯å…¥æ™‚é–“æˆ³
            if 'data_import_timestamp' not in old_df.columns:
                old_df['data_import_timestamp'] = '2024-01-01 00:00:00'  # é è¨­èˆŠæ™‚é–“
            
            # é‡ç½®ç´¢å¼•é¿å…è¡çª
            old_df = old_df.reset_index(drop=True)
            df = df.reset_index(drop=True)
            
            # åˆä½µè³‡æ–™
            combined = pd.concat([old_df, df], ignore_index=True)
            print(f"åˆä½µå¾Œç¸½è¨ˆ: {len(combined)} ç­†")
            
            # æ™ºæ…§å»é‡ï¼šæ ¹æ“š duplicate_key å»é‡ï¼Œä¿ç•™æœ€æ–°çš„è³‡æ–™
            if 'duplicate_key' in combined.columns:
                before_dedup = len(combined)
                
                print(f"å»é‡å‰è©³ç´°åˆ†æ:")
                
                # æª¢æŸ¥ duplicate_key çš„åˆ†å¸ƒ
                dup_counts = combined['duplicate_key'].value_counts()
                unique_keys = len(dup_counts)
                total_duplicates = len(dup_counts[dup_counts > 1])
                
                print(f"  - å”¯ä¸€çš„ duplicate_key: {unique_keys} å€‹")
                print(f"  - æœ‰é‡è¤‡çš„ key: {total_duplicates} å€‹")
                print(f"  - æœ€å¤§é‡è¤‡æ¬¡æ•¸: {dup_counts.max()}")
                
                # æª¢æŸ¥æ™‚é–“æˆ³åˆ†å¸ƒ
                new_data_count = len(combined[combined['data_import_timestamp'] == current_timestamp])
                old_data_count = before_dedup - new_data_count
                print(f"  - æ–°è³‡æ–™æ¨™è¨˜: {new_data_count} ç­†")
                print(f"  - èˆŠè³‡æ–™æ¨™è¨˜: {old_data_count} ç­†")
                
                # è½‰æ›æ™‚é–“æˆ³ç‚º datetime é€²è¡Œæ¯”è¼ƒ
                combined['temp_timestamp'] = pd.to_datetime(combined['data_import_timestamp'], errors='coerce')
                
                # æª¢æŸ¥æ™‚é–“æˆ³è½‰æ›çµæœ
                valid_timestamps = combined['temp_timestamp'].notna().sum()
                print(f"  - æœ‰æ•ˆæ™‚é–“æˆ³: {valid_timestamps} ç­†")
                
                # é¡¯ç¤ºæ™‚é–“æˆ³ç¯„ä¾‹
                timestamp_sample = combined[['duplicate_key', 'data_import_timestamp', 'temp_timestamp']].head(3)
                print(f"  - æ™‚é–“æˆ³ç¯„ä¾‹:")
                for _, row in timestamp_sample.iterrows():
                    print(f"    Key: {row['duplicate_key'][:20]}..., Import: {row['data_import_timestamp']}, Parsed: {row['temp_timestamp']}")
                
                # åˆ†æé‡è¤‡è³‡æ–™
                if total_duplicates > 0:
                    print(f"\né‡è¤‡ key åˆ†æï¼ˆå‰5å€‹ï¼‰:")
                    for key, count in dup_counts.head(5).items():
                        if count > 1:
                            subset = combined[combined['duplicate_key'] == key][['data_import_timestamp', 'temp_timestamp', 'order_sn']]
                            print(f"  Key: {key[:30]}... (é‡è¤‡ {count} æ¬¡)")
                            for _, row in subset.iterrows():
                                print(f"    - {row['order_sn']}: {row['data_import_timestamp']}")
                
                # åŸ·è¡Œå»é‡ï¼šæŒ‰ duplicate_key åˆ†çµ„ï¼Œä¿ç•™æ™‚é–“æˆ³æœ€æ–°çš„è¨˜éŒ„
                print(f"\nåŸ·è¡Œå»é‡...")
                
                # æ–¹æ³•ä¿®æ­£ï¼šä½¿ç”¨ sort_values + drop_duplicates æ›´å®‰å…¨
                combined_sorted = combined.sort_values(['duplicate_key', 'temp_timestamp'], na_position='first')
                combined_deduped = combined_sorted.drop_duplicates(subset=['duplicate_key'], keep='last')
                combined = combined_deduped.drop(columns=['temp_timestamp']).reset_index(drop=True)
                
                after_dedup = len(combined)
                removed_count = before_dedup - after_dedup
                
                print(f"å»é‡çµæœ: {before_dedup} -> {after_dedup} ç­† (ç§»é™¤ {removed_count} ç­†é‡è¤‡)")
                
                # çµ±è¨ˆæœ€çµ‚çµæœ
                final_new_count = len(combined[combined['data_import_timestamp'] == current_timestamp])
                final_old_count = after_dedup - final_new_count
                
                print(f"æœ€çµ‚è³‡æ–™çµ„æˆ:")
                print(f"  - æ–°è³‡æ–™: {final_new_count} ç­†")
                print(f"  - ä¿ç•™èˆŠè³‡æ–™: {final_old_count} ç­†")
                
                # é©—è­‰å»é‡é‚è¼¯
                if removed_count > old_data_count:
                    print(f"âš ï¸  è­¦å‘Š: ç§»é™¤æ•¸é‡({removed_count}) > èˆŠè³‡æ–™æ•¸é‡({old_data_count})ï¼Œå¯èƒ½æœ‰å•é¡Œ")
                
                if final_old_count == 0 and old_data_count > 0:
                    print(f"âš ï¸  è­¦å‘Š: æ‰€æœ‰èˆŠè³‡æ–™éƒ½è¢«ç§»é™¤ï¼Œé€™å¯èƒ½ä¸æ­£å¸¸")
                    
                    # åˆ†æåŸå› 
                    print(f"åˆ†æåŸå› :")
                    if total_duplicates >= unique_keys * 0.8:
                        print(f"  - å¯èƒ½åŸå› : duplicate_key é‡è¤‡ç‡éé«˜ ({total_duplicates}/{unique_keys})")
                    if valid_timestamps < before_dedup * 0.9:
                        print(f"  - å¯èƒ½åŸå› : æ™‚é–“æˆ³è½‰æ›å¤±æ•—ç‡éé«˜")
                
            else:
                print("âš ï¸  è­¦å‘Š: æ‰¾ä¸åˆ° duplicate_key æ¬„ä½ï¼Œè·³éå»é‡")
                
        except Exception as e:
            print(f"è®€å–èˆŠæª”æ¡ˆå¤±æ•—ï¼Œå°‡å»ºç«‹æ–°æª”æ¡ˆ: {e}")
            df['data_import_timestamp'] = current_timestamp
            combined = df
    else:
        df['data_import_timestamp'] = current_timestamp
        combined = df
        print("å»ºç«‹æ–°æª”æ¡ˆ")
    
    # é‡æ–°æ’åºï¼šorder_date å‡åºï¼Œç„¶å¾ŒæŒ‰è¨‚å–®ç·¨è™Ÿæ’åº
    if 'order_date' in combined.columns and 'order_sn' in combined.columns:
        # è™•ç†ç©ºå€¼
        combined['order_date'] = combined['order_date'].fillna('')
        combined['order_sn'] = combined['order_sn'].fillna('')
        
        print(f"æ’åºå‰: {len(combined)} ç­†")
        
        # å…ˆæŒ‰æ—¥æœŸï¼Œå†æŒ‰è¨‚å–®ç·¨è™Ÿæ’åº
        combined = combined.sort_values(['order_date', 'order_sn', 'item_seq']).reset_index(drop=True)
        
        # é‡æ–°è¨ˆç®— item_seqï¼ˆç¢ºä¿åŒä¸€è¨‚å–®å…§çš„å•†å“åºè™Ÿæ­£ç¢ºï¼‰
        combined['item_seq'] = combined.groupby(['order_date', 'order_sn']).cumcount() + 1
        
        # æª¢æŸ¥ç©ºè¨‚å–®
        empty_orders = combined[(combined['order_date'] == '') & (combined['order_sn'] == '')]
        if len(empty_orders) > 0:
            print(f"âš ï¸  ç™¼ç¾ {len(empty_orders)} ç­†ç©ºè¨‚å–®è³‡æ–™ï¼Œå»ºè­°æª¢æŸ¥ä¾†æºæª”æ¡ˆ")
            # å¯é¸ï¼šå°‡ç©ºè¨‚å–®ç§»è‡³æª”æ¡ˆæœ«å°¾
            valid_orders = combined[~((combined['order_date'] == '') & (combined['order_sn'] == ''))]
            combined = pd.concat([valid_orders, empty_orders], ignore_index=True)
        
        print(f"æ’åºå¾Œ: {len(combined)} ç­†")
    
    # ç§»é™¤è‡¨æ™‚æ¬„ä½
    if 'duplicate_key' in combined.columns:
        combined = combined.drop(columns=['duplicate_key'])
    
    # ç¢ºä¿æ¬„ä½é †åºæ­£ç¢ºï¼ˆæŒ‰ç…§ mapping çš„ orderï¼‰
    if mapping:
        # æ¨™æº–æ¬„ä½æŒ‰é †åºæ’åˆ—
        standard_columns = [col for col in combined.columns if col in mapping]
        ordered_columns = sorted(standard_columns, key=lambda x: int(mapping.get(x, {}).get('order', '999')))
        
        # é¡å¤–æ¬„ä½æ·»åŠ åˆ°æœ€å¾Œ
        extra_columns = [col for col in combined.columns if col not in mapping]
        final_columns = ordered_columns + extra_columns
        
        combined = combined[final_columns]
    
    # å„²å­˜æª”æ¡ˆ
    combined.to_csv(OUTPUT_PATH, index=False, encoding='utf-8-sig')
    print(f"âœ… å·²å„²å­˜: {OUTPUT_PATH} ({len(combined)} ç­†)")
    
    # é¡¯ç¤ºè³‡æ–™æ‘˜è¦
    if 'order_date' in combined.columns:
        date_range = combined[combined['order_date'] != '']['order_date'].agg(['min', 'max'])
        if not date_range.empty and date_range['min'] == date_range['min']:  # æª¢æŸ¥é NaN
            print(f"ğŸ“… è³‡æ–™æ—¥æœŸç¯„åœ: {date_range['min']} ~ {date_range['max']}")
    
    if 'data_import_timestamp' in combined.columns:
        latest_import = combined['data_import_timestamp'].max()
        print(f"ğŸ•’ æœ€æ–°åŒ¯å…¥æ™‚é–“: {latest_import}")
    
    print(f"ğŸ“‹ æœ€çµ‚æ¬„ä½é †åº: {list(combined.columns)}")
    return len(combined)

def main():
    print("Shopee CSV è½‰æ›å™¨")
    
    # è®€å–è¨­å®šï¼ˆåŒ…å«ä¸­è‹±æ–‡å°æ‡‰ï¼‰
    mapping, columns, en2zh, zh2en = get_mapping()
    if not mapping:
        print("ç„¡æ³•è¼‰å…¥ mapping è¨­å®šï¼Œç¨‹å¼çµæŸ")
        return
    
    print(f"ğŸ“‹ å·²è¼‰å…¥ {len(zh2en)} å€‹ä¸­è‹±æ–‡æ¬„ä½å°æ‡‰")
    
    # è®€å–æª”æ¡ˆï¼ˆä½¿ç”¨å‹•æ…‹ä¸­è‹±æ–‡å°æ‡‰ï¼‰
    df = read_csv_files(zh2en)
    if df.empty:
        print("æ²’æœ‰æ‰¾åˆ° CSV æª”æ¡ˆ")
        return
    
    print(f"\nğŸ“Š è®€å– {len(df)} ç­†è³‡æ–™")
    
    # è™•ç†è³‡æ–™
    processed_df = process_data(df, mapping, columns)
    print(f"âœ… è™•ç†å®Œæˆ {len(processed_df)} ç­†")
    
    # å„²å­˜
    final_count = save_data(processed_df, mapping)
    
    # æ¸…ç† temp æª”æ¡ˆ
    try:
        for f in os.listdir(SOURCE_DIR):
            if f.lower().endswith('.csv'):
                os.unlink(os.path.join(SOURCE_DIR, f))
        print("ğŸ§¹ å·²æ¸…ç†è‡¨æ™‚æª”æ¡ˆ")
    except Exception as e:
        print(f"âš ï¸  æ¸…ç†è‡¨æ™‚æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    print(f"ğŸ‰ å®Œæˆï¼æœ€çµ‚è³‡æ–™: {final_count} ç­†")

if __name__ == "__main__":
    main()