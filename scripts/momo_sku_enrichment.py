import os
import pandas as pd
import json
import glob
from datetime import datetime
from pathlib import Path

# è·¯å¾‘è¨­å®š
PROJECT_ROOT = Path(__file__).parent.parent
DATA_PROCESSED_DIR = PROJECT_ROOT / 'data_processed' / 'merged'
CONFIG_DIR = PROJECT_ROOT / 'config'
LOG_DIR = PROJECT_ROOT / 'logs'

# ç¢ºä¿ç›®éŒ„å­˜åœ¨
os.makedirs(LOG_DIR, exist_ok=True)

def load_sku_mapping():
    """è¼‰å…¥ SKU mapping è³‡æ–™"""
    mapping_file = CONFIG_DIR / 'sku_mapping.json'
    
    if not mapping_file.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° SKU mapping æª”æ¡ˆï¼š{mapping_file}")
    
    with open(mapping_file, 'r', encoding='utf-8') as f:
        mapping_data = json.load(f)
    
    print(f"âœ… æˆåŠŸè¼‰å…¥ SKU mappingï¼ŒåŒ…å« {len(mapping_data['barcode_mapping'])} ç­†æ¢ç¢¼è³‡æ–™")
    return mapping_data

def find_momo_files():
    """å°‹æ‰¾ momo CSV æª”æ¡ˆ"""
    pattern = DATA_PROCESSED_DIR / 'momo_*.csv'
    files = glob.glob(str(pattern))
    
    # æ’é™¤å·²ç¶“è™•ç†éçš„ _enriched.csv æª”æ¡ˆ
    files = [f for f in files if not Path(f).name.endswith('_enriched.csv')]
    
    if not files:
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° momo CSV æª”æ¡ˆæ–¼ {DATA_PROCESSED_DIR}")
    
    print(f"ğŸ“ æ‰¾åˆ° {len(files)} å€‹ momo CSV æª”æ¡ˆï¼š")
    for f in files:
        print(f"   - {Path(f).name}")
    
    return files

def get_barcode_from_row(row, mapping_data):
    """å¾è³‡æ–™è¡Œä¸­å–å¾—æ¢ç¢¼ï¼Œä¸¦å›å‚³å°æ‡‰çš„å•†å“è³‡è¨Š"""
    # å˜—è©¦ä¸åŒçš„æ¢ç¢¼æ¬„ä½
    barcode_fields = ['product_manufacturer_code', 'barcode', 'product_sku_main']
    
    for field in barcode_fields:
        if field in row and pd.notna(row[field]) and str(row[field]).strip() != '':
            barcode = str(row[field]).strip()
            
            # åœ¨ mapping ä¸­å°‹æ‰¾å°æ‡‰çš„å•†å“è³‡è¨Š
            if barcode in mapping_data['barcode_mapping']:
                return mapping_data['barcode_mapping'][barcode]
    
    return None

def enrich_momo_data(file_path, mapping_data):
    """ç‚º momo è³‡æ–™å¢åŠ æ¬„ä½"""
    print(f"\nğŸ“– è™•ç†æª”æ¡ˆï¼š{Path(file_path).name}")
    
    # è®€å– CSV æª”æ¡ˆ
    df = pd.read_csv(file_path, dtype=str)
    print(f"ğŸ“Š åŸå§‹è³‡æ–™ç­†æ•¸ï¼š{len(df)}")
    
    # è¦å¢åŠ çš„æ¬„ä½ï¼ˆç§»é™¤æŒ‡å®šçš„æ¬„ä½ï¼‰
    new_columns = [
        'category', 'subcategory', 'brand', 'series', 'pet_type', 
        'product_name_mapped', 'item_code', 'sku', 'tags', 'spec', 
        'unit', 'package_type', 'package_qty', 'origin', 'cost', 
        'supplier_code', 'supplier', 'supplier_ref'
    ]
    
    # åˆå§‹åŒ–æ–°æ¬„ä½
    for col in new_columns:
        df[col] = ''
    
    # çµ±è¨ˆè³‡è¨Š
    matched_count = 0
    unmatched_count = 0
    
    # é€è¡Œè™•ç†
    for idx, row in df.iterrows():
        product_info = get_barcode_from_row(row, mapping_data)
        
        if product_info:
            matched_count += 1
            # å¡«å…¥å°æ‡‰çš„å•†å“è³‡è¨Š
            for col in new_columns:
                if col in product_info and product_info[col] is not None:
                    df.at[idx, col] = str(product_info[col])
        else:
            unmatched_count += 1
    
    print(f"âœ… åŒ¹é…æˆåŠŸï¼š{matched_count} ç­†")
    print(f"âŒ æœªåŒ¹é…ï¼š{unmatched_count} ç­†")
    print(f"ğŸ“ˆ åŒ¹é…ç‡ï¼š{matched_count/(matched_count+unmatched_count)*100:.1f}%")
    
    return df

def save_enriched_data(df, original_file_path):
    """å„²å­˜å¢åŠ æ¬„ä½å¾Œçš„è³‡æ–™"""
    # å»ºç«‹æ–°çš„æª”æ¡ˆåç¨±
    original_path = Path(original_file_path)
    new_filename = f"{original_path.stem}_enriched{original_path.suffix}"
    output_path = original_path.parent / new_filename
    
    # å„²å­˜æª”æ¡ˆ
    df.to_csv(output_path, index=False, encoding='utf-8-sig')
    print(f"ğŸ’¾ å·²å„²å­˜ï¼š{output_path.name}")
    
    return output_path

def main():
    """ä¸»è¦è™•ç†å‡½æ•¸"""
    try:
        print("ğŸš€ é–‹å§‹è™•ç† momo è³‡æ–™å¢åŠ æ¬„ä½...")
        
        # è¼‰å…¥ SKU mapping
        mapping_data = load_sku_mapping()
        
        # å°‹æ‰¾ momo æª”æ¡ˆ
        momo_files = find_momo_files()
        
        # è™•ç†æ¯å€‹æª”æ¡ˆ
        processed_files = []
        total_matched = 0
        total_unmatched = 0
        
        for file_path in momo_files:
            try:
                # å¢åŠ æ¬„ä½
                enriched_df = enrich_momo_data(file_path, mapping_data)
                
                # å„²å­˜çµæœ
                output_path = save_enriched_data(enriched_df, file_path)
                processed_files.append(output_path)
                
                # çµ±è¨ˆåŒ¹é…æ•¸é‡
                matched_count = len(enriched_df[enriched_df['sku'] != ''])
                unmatched_count = len(enriched_df[enriched_df['sku'] == ''])
                total_matched += matched_count
                total_unmatched += unmatched_count
                
            except Exception as e:
                print(f"âŒ è™•ç†æª”æ¡ˆ {Path(file_path).name} æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                continue
        
        # è¼¸å‡ºç¸½çµ
        print(f"\nğŸ‰ è™•ç†å®Œæˆï¼")
        print(f"ğŸ“ è™•ç†æª”æ¡ˆæ•¸ï¼š{len(processed_files)}")
        print(f"ğŸ“Š ç¸½åŒ¹é…ç­†æ•¸ï¼š{total_matched}")
        print(f"ğŸ“Š ç¸½æœªåŒ¹é…ç­†æ•¸ï¼š{total_unmatched}")
        print(f"ğŸ“ˆ æ•´é«”åŒ¹é…ç‡ï¼š{total_matched/(total_matched+total_unmatched)*100:.1f}%")
        
        # å¯«å…¥ log
        log_file = LOG_DIR / 'momo_sku_enrichment.log'
        with open(log_file, 'a', encoding='utf-8') as f:
            f.write(f"{datetime.now().isoformat()} - è™•ç†å®Œæˆ\n")
            f.write(f"  è™•ç†æª”æ¡ˆæ•¸ï¼š{len(processed_files)}, ç¸½åŒ¹é…ï¼š{total_matched}, ç¸½æœªåŒ¹é…ï¼š{total_unmatched}\n")
        
        print(f"ğŸ“ è©³ç´°è¨˜éŒ„å·²å¯«å…¥ï¼š{log_file}")
        
    except Exception as e:
        print(f"âŒ éŒ¯èª¤ï¼š{e}")
        # å¯«å…¥éŒ¯èª¤ log
        log_file = LOG_DIR / 'momo_sku_enrichment.log'
        with open(log_file, 'a', encoding='utf-8') as f:
            f.write(f"{datetime.now().isoformat()} - éŒ¯èª¤ï¼š{e}\n")

if __name__ == '__main__':
    main() 