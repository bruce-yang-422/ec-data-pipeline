#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ETMall BigQuery ä¸Šå‚³å™¨

ä¸»è¦åŠŸèƒ½ï¼š
- å°ˆé–€ä¸Šå‚³ ETMall è¨‚å–®è³‡æ–™è‡³ BigQuery
- è‡ªå‹•æŠ“å–æœ€æ–°çš„ ETMall CSV æª”æ¡ˆ
- ä¸Šå‚³åˆ° shopee-etl-reporting.yichai_etmall_data.etmall_orders_data
- è‡ªå‹•é‡è¤‡è³‡æ–™æª¢æŸ¥èˆ‡è™•ç†
- å®Œæ•´çš„æ—¥èªŒè¨˜éŒ„èˆ‡éŒ¯èª¤è¿½è¹¤
- æ”¯æ´å¤šç¨®ä¸Šå‚³æ¨¡å¼ (WRITE_TRUNCATE/APPEND/EMPTY)ï¼Œé è¨­ç‚ºè¦†è“‹æ¨¡å¼
- æ ¹æ“š etmall_fields_mapping.json å‹•æ…‹ç”Ÿæˆ schema

Authors: æ¥Šç¿”å¿— & AI Collective
Studio: tranquility-base
"""

import argparse
import os
import sys
import pandas as pd
import logging
import glob
import json
from datetime import datetime
from pathlib import Path

# âœ… å°‡å°ˆæ¡ˆæ ¹ç›®éŒ„åŠ å…¥ sys.path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))

# âœ… ä½¿ç”¨ç›¸å° import
from bigquery_utils import get_bq_client, upload_csv_to_bq, check_duplicate_order_sn
from google.cloud import bigquery

# ETMall å°ˆç”¨è¨­å®š
ETMALL_DATASET = "yichai_etmall_data"
ETMALL_TABLE = "etmall_orders_data"
ETMALL_PROJECT = "shopee-etl-reporting"

def get_csv_pattern():
    """æ ¹æ“šç•¶å‰å·¥ä½œç›®éŒ„å‹•æ…‹è¨­å®š CSV æª”æ¡ˆè·¯å¾‘"""
    current_dir = os.getcwd()
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    # å¦‚æœç•¶å‰ç›®éŒ„æ˜¯å°ˆæ¡ˆæ ¹ç›®éŒ„
    if os.path.basename(current_dir) == "ec-data-pipeline":
        return "data_processed/merged/06_etmall_orders_bq_formatted_*.csv"
    # å¦‚æœç•¶å‰ç›®éŒ„æ˜¯ scripts/bigquery_uploader
    elif os.path.basename(current_dir) == "bigquery_uploader":
        return "../../data_processed/merged/06_etmall_orders_bq_formatted_*.csv"
    # å…¶ä»–æƒ…æ³ï¼Œå˜—è©¦ç›¸å°è·¯å¾‘
    else:
        return "../../data_processed/merged/06_etmall_orders_bq_formatted_*.csv"

def get_credential_path():
    """æ ¹æ“šç•¶å‰å·¥ä½œç›®éŒ„å‹•æ…‹è¨­å®šèªè­‰æª”æ¡ˆè·¯å¾‘"""
    current_dir = os.getcwd()
    
    # å¦‚æœç•¶å‰ç›®éŒ„æ˜¯å°ˆæ¡ˆæ ¹ç›®éŒ„
    if os.path.basename(current_dir) == "ec-data-pipeline":
        return "config/bigquery_uploader_key.json"
    # å¦‚æœç•¶å‰ç›®éŒ„æ˜¯ scripts/bigquery_uploader
    elif os.path.basename(current_dir) == "bigquery_uploader":
        return "../../config/bigquery_uploader_key.json"
    # å…¶ä»–æƒ…æ³ï¼Œå˜—è©¦ç›¸å°è·¯å¾‘
    else:
        return "../../config/bigquery_uploader_key.json"

def get_mapping_path():
    """æ ¹æ“šç•¶å‰å·¥ä½œç›®éŒ„å‹•æ…‹è¨­å®šæ¬„ä½æ˜ å°„æª”æ¡ˆè·¯å¾‘"""
    current_dir = os.getcwd()
    
    # å¦‚æœç•¶å‰ç›®éŒ„æ˜¯å°ˆæ¡ˆæ ¹ç›®éŒ„
    if os.path.basename(current_dir) == "ec-data-pipeline":
        return "config/etmall_fields_mapping.json"
    # å¦‚æœç•¶å‰ç›®éŒ„æ˜¯ scripts/bigquery_uploader
    elif os.path.basename(current_dir) == "bigquery_uploader":
        return "../../config/etmall_fields_mapping.json"
    # å…¶ä»–æƒ…æ³ï¼Œå˜—è©¦ç›¸å°è·¯å¾‘
    else:
        return "../../config/etmall_fields_mapping.json"

def load_field_mapping():
    """è¼‰å…¥æ¬„ä½æ˜ å°„é…ç½®"""
    mapping_path = get_mapping_path()
    try:
        with open(mapping_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ¬„ä½æ˜ å°„æª”æ¡ˆï¼š{mapping_path}")
    except json.JSONDecodeError as e:
        raise ValueError(f"æ¬„ä½æ˜ å°„æª”æ¡ˆæ ¼å¼éŒ¯èª¤ï¼š{e}")

def generate_schema_from_mapping(field_mapping: dict[str, dict[str, str]]) -> list[bigquery.SchemaField]:
    """æ ¹æ“šæ¬„ä½æ˜ å°„ç”Ÿæˆ BigQuery Schema"""
    schema: list[bigquery.SchemaField] = []
    
    # BigQuery è³‡æ–™é¡å‹æ˜ å°„
    type_mapping = {
        "String": "STRING",
        "Integer": "INTEGER", 
        "Float": "FLOAT",
        "Date": "DATE",
        "Datetime": "DATETIME",
        "Text": "STRING",
        "Boolean": "BOOLEAN"
    }
    
    # æŒ‰ order æ¬„ä½æ’åº
    sorted_fields = sorted(field_mapping.items(), key=lambda x: int(x[1]['order']))
    
    for field_name, field_info in sorted_fields:
        bq_type = type_mapping.get(field_info['type'], 'STRING')
        # ç‚ºäº†é¿å…è³‡æ–™ä¸Šå‚³å•é¡Œï¼Œæ‰€æœ‰æ¬„ä½éƒ½è¨­ç‚º NULLABLE
        mode = 'NULLABLE'
        
        schema.append(bigquery.SchemaField(field_name, bq_type, mode=mode))
    
    # æ·»åŠ  processing_date æ¬„ä½
    schema.append(bigquery.SchemaField("processing_date", "TIMESTAMP", mode="NULLABLE"))
    
    return schema

def find_latest_etmall_csv():
    """è‡ªå‹•æŠ“å–æœ€æ–°çš„ ETMall CSV æª”æ¡ˆ"""
    csv_files = glob.glob(get_csv_pattern())
    if not csv_files:
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°ç¬¦åˆæ¨¡å¼çš„ CSV æª”æ¡ˆï¼š{get_csv_pattern()}")
    
    # æ¨™æº–åŒ–è·¯å¾‘ä¸¦æŒ‰æª”æ¡ˆä¿®æ”¹æ™‚é–“æ’åºï¼Œå–æœ€æ–°çš„
    csv_files = [os.path.normpath(f) for f in csv_files]
    latest_file = max(csv_files, key=os.path.getmtime)
    return latest_file

def setup_logging():
    """è¨­å®šæ—¥èªŒ"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = f"logs/etmall_to_bigquery_uploader_{timestamp}.log"
    
    # ç¢ºä¿ logs ç›®éŒ„å­˜åœ¨
    os.makedirs("logs", exist_ok=True)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filename, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)

def validate_csv_columns(df: pd.DataFrame, field_mapping: dict[str, dict[str, str]]) -> bool:
    """é©—è­‰ CSV æª”æ¡ˆçš„æ¬„ä½æ˜¯å¦ç¬¦åˆæ˜ å°„é…ç½®"""
    csv_columns = set(df.columns)
    mapping_columns = set(field_mapping.keys())
    
    missing_columns = mapping_columns - csv_columns
    extra_columns = csv_columns - mapping_columns
    
    if missing_columns:
        logging.warning(f"CSV æª”æ¡ˆç¼ºå°‘ä»¥ä¸‹æ¬„ä½ï¼š{missing_columns}")
    
    if extra_columns:
        logging.info(f"CSV æª”æ¡ˆåŒ…å«é¡å¤–æ¬„ä½ï¼š{extra_columns}")
    
    return len(missing_columns) == 0

def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description="ETMall è¨‚å–®è³‡æ–™ä¸Šå‚³è‡³ BigQuery")
    parser.add_argument("--credential", default=get_credential_path(),
                       help="GCP èªè­‰ JSON æª”æ¡ˆè·¯å¾‘")
    parser.add_argument("--csv", help="CSV æª”æ¡ˆè·¯å¾‘ï¼ˆä¸æŒ‡å®šå‰‡è‡ªå‹•æŠ“å–æœ€æ–°æª”æ¡ˆï¼‰")
    parser.add_argument("--project", default=ETMALL_PROJECT, help="BigQuery å°ˆæ¡ˆ ID")
    parser.add_argument("--dataset", default=ETMALL_DATASET, help="BigQuery Dataset åç¨±")
    parser.add_argument("--table", default=ETMALL_TABLE, help="BigQuery Table åç¨±")
    parser.add_argument("--write_disposition", default="WRITE_TRUNCATE",
                       choices=["WRITE_TRUNCATE", "WRITE_APPEND", "WRITE_EMPTY"],
                       help="ä¸Šå‚³æ¨¡å¼")
    parser.add_argument("--check_duplicates", action="store_true", default=True,
                       help="æª¢æŸ¥ order_sn é‡è¤‡")
    parser.add_argument("--no_check_duplicates", action="store_true",
                       help="è·³éé‡è¤‡æª¢æŸ¥")
    
    args = parser.parse_args()
    
    # è¨­å®šæ—¥èªŒ
    logger = setup_logging()
    logger.info("=== ETMall è¨‚å–®è³‡æ–™ä¸Šå‚³è‡³ BigQuery ===")
    logger.info(f"ç›®æ¨™è³‡æ–™è¡¨ï¼š{args.project}.{args.dataset}.{args.table}")
    
    # è¼‰å…¥æ¬„ä½æ˜ å°„
    try:
        field_mapping = load_field_mapping()
        logger.info("âœ… æ¬„ä½æ˜ å°„è¼‰å…¥æˆåŠŸ")
    except Exception as e:
        logger.error(f"âŒ è¼‰å…¥æ¬„ä½æ˜ å°„å¤±æ•—ï¼š{e}")
        return 1
    
    # ç”Ÿæˆ schema
    schema = generate_schema_from_mapping(field_mapping)
    logger.info(f"âœ… å·²ç”Ÿæˆ BigQuery Schemaï¼ŒåŒ…å« {len(schema)} å€‹æ¬„ä½")
    
    # è‡ªå‹•æŠ“å–æœ€æ–°çš„ CSV æª”æ¡ˆ
    if args.csv:
        csv_path = args.csv
        logger.info(f"ä½¿ç”¨æŒ‡å®š CSV æª”æ¡ˆï¼š{csv_path}")
    else:
        try:
            csv_path = find_latest_etmall_csv()
            logger.info(f"è‡ªå‹•æŠ“å–æœ€æ–° CSV æª”æ¡ˆï¼š{csv_path}")
        except FileNotFoundError as e:
            logger.error(f"âŒ éŒ¯èª¤ï¼š{e}")
            return 1
    
    # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
    if not os.path.exists(csv_path):
        logger.error(f"âŒ CSV æª”æ¡ˆä¸å­˜åœ¨ï¼š{csv_path}")
        return 1
    
    # æª¢æŸ¥é‡è¤‡è¨­å®š
    check_duplicates = args.check_duplicates and not args.no_check_duplicates
    
    try:
        # å»ºç«‹ BigQuery å®¢æˆ¶ç«¯
        client = get_bq_client(args.credential)
        logger.info("âœ… BigQuery å®¢æˆ¶ç«¯å»ºç«‹æˆåŠŸ")
        
        # è®€å– CSV æª”æ¡ˆ
        logger.info("ğŸ“– è®€å– CSV æª”æ¡ˆ...")
        df = pd.read_csv(csv_path, dtype=str)
        logger.info(f"âœ… CSV æª”æ¡ˆè®€å–æˆåŠŸï¼Œå…± {len(df)} ç­†è³‡æ–™")
        
        # é©—è­‰æ¬„ä½
        if not validate_csv_columns(df, field_mapping):
            logger.warning("âš ï¸ CSV æª”æ¡ˆæ¬„ä½èˆ‡æ˜ å°„é…ç½®ä¸å®Œå…¨åŒ¹é…")
        
        # æª¢æŸ¥é‡è¤‡è³‡æ–™
        if check_duplicates:
            logger.info("ğŸ” æª¢æŸ¥é‡è¤‡è³‡æ–™...")
            duplicate_sns = check_duplicate_order_sn(df)
            if duplicate_sns:
                logger.info(f"ç™¼ç¾ {len(duplicate_sns)} å€‹é‡è¤‡çš„ order_sn")
            else:
                logger.info("ç„¡é‡è¤‡çš„ order_sn")
        
        # ç‚º CSV æª”æ¡ˆæ·»åŠ  processing_date æ¬„ä½
        logger.info("ğŸ“ ç‚º CSV æª”æ¡ˆæ·»åŠ  processing_date æ¬„ä½...")
        df['processing_date'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        # ä¿®æ­£è³‡æ–™é¡å‹å•é¡Œ
        logger.info("ğŸ”§ ä¿®æ­£è³‡æ–™é¡å‹...")
        
        # æ ¹æ“šæ˜ å°„é…ç½®è™•ç†è³‡æ–™é¡å‹
        for field_name, field_info in field_mapping.items():
            if field_name in df.columns:
                field_type = field_info['type']
                
                if field_type == 'Float':
                    # è™•ç†æµ®é»æ•¸æ¬„ä½
                    df[field_name] = df[field_name].fillna('0').astype(float).astype(str)
                elif field_type == 'Integer':
                    # è™•ç†æ•´æ•¸æ¬„ä½
                    df[field_name] = df[field_name].fillna('0').astype(float).astype(int).astype(str)
                elif field_type in ['Date', 'Datetime']:
                    # è™•ç†æ—¥æœŸæ¬„ä½ï¼Œä¿æŒå­—ä¸²æ ¼å¼
                    df[field_name] = df[field_name].fillna('').astype(str)
                else:
                    # å…¶ä»–æ¬„ä½ä¿æŒå­—ä¸²æ ¼å¼ï¼Œé¿å…ç§‘å­¸è¨˜è™Ÿå•é¡Œ
                    df[field_name] = df[field_name].fillna('').astype(str).str.replace('.0', '', regex=False).str.replace('nan', '', regex=False)
        
        # å‰µå»ºè‡¨æ™‚æª”æ¡ˆ
        temp_csv_path = csv_path.replace('.csv', '_with_processing_date.csv')
        df.to_csv(temp_csv_path, index=False)
        logger.info(f"âœ… å·²å‰µå»ºè‡¨æ™‚æª”æ¡ˆï¼š{temp_csv_path}")
        
        # ä¸Šå‚³è³‡æ–™
        logger.info(f"ğŸ“¤ é–‹å§‹ä¸Šå‚³è³‡æ–™...")
        logger.info(f"æª”æ¡ˆï¼š{temp_csv_path}")
        logger.info(f"æ¨¡å¼ï¼š{args.write_disposition}")
        
        result = upload_csv_to_bq(
            client=client,
            csv_path=temp_csv_path,
            dataset_id=args.dataset,
            table_id=args.table,
            schema=schema,
            write_disposition=args.write_disposition
        )
        
        # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
        if os.path.exists(temp_csv_path):
            os.remove(temp_csv_path)
            logger.info("ğŸ§¹ å·²æ¸…ç†è‡¨æ™‚æª”æ¡ˆ")
        
        if result:
            logger.info("âœ… è³‡æ–™ä¸Šå‚³æˆåŠŸï¼")
            logger.info(f"ä¸Šå‚³ç­†æ•¸ï¼š{result}")
            return 0
        else:
            logger.error("âŒ è³‡æ–™ä¸Šå‚³å¤±æ•—")
            return 1
            
    except Exception as e:
        logger.error(f"âŒ ä¸Šå‚³éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
        return 1



if __name__ == "__main__":
    # ç›´æ¥ä½¿ç”¨ä¸»å‡½æ•¸ï¼Œé è¨­ç‚ºè¦†è“‹æ¨¡å¼
    exit(main())
