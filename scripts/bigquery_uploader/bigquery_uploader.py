#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BigQuery CSV ä¸Šå‚³å™¨

ä¸»è¦åŠŸèƒ½ï¼š
- æ”¯æ´å–®ä¸€æª”æ¡ˆæˆ–æ‰¹æ¬¡ä¸Šå‚³è‡³ BigQuery
- è‡ªå‹•é‡è¤‡è³‡æ–™æª¢æŸ¥èˆ‡è™•ç†
- äº’å‹•å¼æ“ä½œä»‹é¢
- å®Œæ•´çš„æ—¥èªŒè¨˜éŒ„èˆ‡éŒ¯èª¤è¿½è¹¤
- æ”¯æ´å¤šç¨®ä¸Šå‚³æ¨¡å¼ (WRITE_TRUNCATE/APPEND/EMPTY)

Authors: æ¥Šç¿”å¿— & AI Collective
Studio: tranquility-base
"""

import argparse
import os
import sys
import pandas as pd
import logging
from datetime import datetime
from pathlib import Path

# âœ… å°‡å°ˆæ¡ˆæ ¹ç›®éŒ„åŠ å…¥ sys.path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))

# âœ… ä½¿ç”¨ç›¸å° import
from bigquery_utils import get_bq_client, upload_csv_to_bq, check_duplicate_order_sn
from bq_schemas import (
    c1105_momo_accounting_orders_schema,
    a1102_momo_shipping_orders_schema,
    etmall_orders_schema,
)

# é è¨­ dataset èˆ‡ cleaned æª”æ¡ˆè·¯å¾‘
DEFAULT_DATASET = "yichai_momo_data"
DEFAULT_FILES = {
    "c1105_momo_accounting_orders": "data_processed/merged/momo_accounting_orders_deduplicated.csv",
    "a1102_momo_shipping_orders": "data_processed/merged/momo_shipping_orders_deduplicated.csv",
    "etmall_orders": "data_processed/merged/etmall_orders_bq_formatted_20250807_115715.csv",
}


def setup_logging():
    """è¨­å®š BigQuery ä¸Šå‚³å™¨çš„æ—¥èªŒç³»çµ±"""
    # å–å¾—å°ˆæ¡ˆæ ¹ç›®éŒ„
    script_dir = Path(__file__).parent
    project_root = script_dir.parents[1]  # å‘ä¸Šå…©å±¤åˆ°é”å°ˆæ¡ˆæ ¹ç›®éŒ„
    logs_dir = project_root / "logs"
    
    # ç¢ºä¿ logs ç›®éŒ„å­˜åœ¨
    logs_dir.mkdir(parents=True, exist_ok=True)
    
    # è¨­å®šæ—¥èªŒæª”æ¡ˆåç¨±
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = f"bigquery_uploader_{timestamp}.log"
    log_path = logs_dir / log_filename
    
    # è¨­å®šæ—¥èªŒæ ¼å¼
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_path, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info("=== BigQuery ä¸Šå‚³å™¨é–‹å§‹ ===")
    logger.info(f"å°ˆæ¡ˆæ ¹ç›®éŒ„ï¼š{project_root}")
    logger.info(f"æ—¥èªŒæª”æ¡ˆï¼š{log_path}")
    
    return logger


def main():
    """BigQuery ä¸Šå‚³å™¨ä¸»ç¨‹å¼å…¥å£é»"""
    # è¨­å®šæ—¥èªŒ
    logger = setup_logging()
    
    parser = argparse.ArgumentParser(description="é€šç”¨ BigQuery CSV ä¸Šå‚³å™¨")
    parser.add_argument("--credential", type=str, default="config/bigquery_uploader_key.json", help="GCP èªè­‰ JSON è·¯å¾‘")
    parser.add_argument("--csv", type=str, help="CSV æª”æ¡ˆè·¯å¾‘ (å¦‚ä¸æŒ‡å®šï¼Œå°‡ä½¿ç”¨é è¨­è·¯å¾‘)")
    parser.add_argument("--dataset", type=str, default=DEFAULT_DATASET, help=f"BigQuery Dataset åç¨± (é è¨­: {DEFAULT_DATASET})")
    parser.add_argument("--table", type=str, help="BigQuery Table åç¨±")
    parser.add_argument("--write_disposition", type=str, choices=["WRITE_TRUNCATE", "WRITE_APPEND", "WRITE_EMPTY"], default="WRITE_APPEND", help="ä¸Šå‚³æ¨¡å¼")
    parser.add_argument("--check_duplicates", action="store_true", default=True, help="æª¢æŸ¥ order_sn é‡è¤‡ (é è¨­é–‹å•Ÿ)")
    parser.add_argument("--no_check_duplicates", action="store_true", help="è·³éé‡è¤‡æª¢æŸ¥")
    parser.add_argument("--upload_all", action="store_true", help="ä¸€éµä¸Šå‚³æ‰€æœ‰é è¨­ cleaned æª”æ¡ˆ")
    args = parser.parse_args()

    check_duplicates = args.check_duplicates and not args.no_check_duplicates
    logger.info(f"åƒæ•¸è¨­å®šï¼šdataset={args.dataset}, write_disposition={args.write_disposition}, check_duplicates={check_duplicates}")

    if args.upload_all:
        logger.info("ğŸš€ é–‹å§‹ä¸Šå‚³æ‰€æœ‰é è¨­ cleaned æª”æ¡ˆ...")
        for table_name, csv_path in DEFAULT_FILES.items():
            upload_single_file(args.credential, csv_path, args.dataset, table_name, args.write_disposition, check_duplicates, logger)
        logger.info("âœ… æ‰€æœ‰æª”æ¡ˆä¸Šå‚³å®Œæˆï¼")
        return

    if not args.table:
        logger.error("âŒ å–®ä¸€ä¸Šå‚³æ¨¡å¼éœ€æŒ‡å®š --table")
        return

    csv_path = args.csv
    if not csv_path:
        csv_path = DEFAULT_FILES.get(args.table)
        if not csv_path:
            logger.error(f"âŒ æœªæ‰¾åˆ° table {args.table} çš„é è¨­æª”æ¡ˆè·¯å¾‘")
            return
        logger.info(f"ğŸ“‚ ä½¿ç”¨é è¨­è·¯å¾‘: {csv_path}")

    upload_single_file(args.credential, csv_path, args.dataset, args.table, args.write_disposition, check_duplicates, logger)


def upload_single_file(credential_path: str, csv_path: str, dataset_id: str, table_id: str, write_disposition: str, check_duplicates: bool = True, logger=None) -> None:
    """ä¸Šå‚³å–®ä¸€ CSV æª”æ¡ˆè‡³ BigQuery"""
    csv_path = csv_path.replace('/', os.sep)

    if not os.path.exists(csv_path):
        error_msg = f"âŒ æ‰¾ä¸åˆ° CSV: {csv_path}"
        if logger:
            logger.error(error_msg)
        else:
            print(error_msg)
        return

    schema_dict = {
        "c1105_momo_accounting_orders": c1105_momo_accounting_orders_schema,
        "a1102_momo_shipping_orders": a1102_momo_shipping_orders_schema,
        "etmall_orders": etmall_orders_schema,
    }

    schema = schema_dict.get(table_id)
    if schema is None:
        error_msg = f"âŒ å°šæœªå®šç¾© table schema: {table_id}"
        if logger:
            logger.error(error_msg)
        else:
            print(error_msg)
        return

    info_msg = f"ğŸ“¤ æº–å‚™ä¸Šå‚³: {csv_path} -> {dataset_id}.{table_id}"
    if logger:
        logger.info(info_msg)
    else:
        print(info_msg)

    client = get_bq_client(credential_path)

    df = pd.read_csv(csv_path, dtype=str)

    final_write_disposition = write_disposition
    if check_duplicates:
        check_msg = "ğŸ” æª¢æŸ¥ order_sn é‡è¤‡..."
        if logger:
            logger.info(check_msg)
        else:
            print(check_msg)
        
        duplicates = check_duplicate_order_sn(df)
        if duplicates is not None:
            duplicate_count = len(duplicates)
            warning_msg = f"âš ï¸ ç™¼ç¾ {duplicate_count} ç­†é‡è¤‡ order_sn"
            if logger:
                logger.warning(warning_msg)
            else:
                print(warning_msg)
            
            if write_disposition == "WRITE_APPEND":
                final_write_disposition = "WRITE_TRUNCATE"
                switch_msg = "ğŸ”„ è‡ªå‹•åˆ‡æ›ç‚ºè¦†è“‹æ¨¡å¼ (WRITE_TRUNCATE)"
                if logger:
                    logger.info(switch_msg)
                else:
                    print(switch_msg)
            else:
                mode_msg = f"ğŸ’¡ ç¶­æŒåŸè¨­å®šæ¨¡å¼: {write_disposition}"
                if logger:
                    logger.info(mode_msg)
                else:
                    print(mode_msg)
        else:
            success_msg = "âœ… ç„¡é‡è¤‡ order_sn"
            if logger:
                logger.info(success_msg)
            else:
                print(success_msg)
    else:
        skip_msg = "â„¹ï¸ è·³éé‡è¤‡æª¢æŸ¥"
        if logger:
            logger.info(skip_msg)
        else:
            print(skip_msg)

    mode_msg = f"ğŸ“Š ä½¿ç”¨æ¨¡å¼: {final_write_disposition}"
    if logger:
        logger.info(mode_msg)
    else:
        print(mode_msg)

    upload_csv_to_bq(
        client,
        csv_path,
        dataset_id,
        table_id,
        schema,
        write_disposition=final_write_disposition,
        logger=logger
    )


def interactive_mode():
    """äº’å‹•å¼ BigQuery ä¸Šå‚³æ¨¡å¼"""
    # è¨­å®šæ—¥èªŒ
    logger = setup_logging()
    
    print("=== BigQuery äº’å‹•å¼ä¸Šå‚³ï¼ˆå¤šå¹³å°è¨‚å–®æ•¸æ“šï¼‰===")
    print("è«‹é¸æ“‡è¦ä¸Šå‚³çš„æª”æ¡ˆï¼š")
    print("1. c1105_momo_accounting_orders (Momo è¨‚å–®å¸³å‹™)")
    print("2. a1102_momo_shipping_orders (Momo è¨‚å–®å‡ºè²¨)")
    print("3. etmall_orders (ETMall è¨‚å–®)")
    print("4. å…¨éƒ¨ä¸Šå‚³")
    
    choice = input("è«‹è¼¸å…¥æ•¸å­—é¸æ“‡ [1/2/3/4]ï¼š").strip()
    
    # å›ºå®šåƒæ•¸
    credential = "config/bigquery_uploader_key.json"
    dataset = "yichai_momo_data"
    write_disposition = "WRITE_TRUNCATE"  # å›ºå®šä½¿ç”¨ WRITE_TRUNCATE
    check_duplicates = True  # å›ºå®šæª¢æŸ¥é‡è¤‡
    
    logger.info(f"ç”¨æˆ¶é¸æ“‡: {choice}")
    logger.info(f"å›ºå®šåƒæ•¸: credential={credential}, dataset={dataset}, write_disposition={write_disposition}, check_duplicates={check_duplicates}")
    
    if choice == "1":
        table = "c1105_momo_accounting_orders"
        csv = "data_processed/merged/momo_accounting_orders_deduplicated.csv"
        logger.info(f"æº–å‚™ä¸Šå‚³: {csv} -> {dataset}.{table}")
        print(f"\næº–å‚™ä¸Šå‚³: {csv} -> {dataset}.{table}")
        upload_single_file(credential, csv, dataset, table, write_disposition, check_duplicates, logger)
    elif choice == "2":
        table = "a1102_momo_shipping_orders"
        csv = "data_processed/merged/momo_shipping_orders_deduplicated.csv"
        logger.info(f"æº–å‚™ä¸Šå‚³: {csv} -> {dataset}.{table}")
        print(f"\næº–å‚™ä¸Šå‚³: {csv} -> {dataset}.{table}")
        upload_single_file(credential, csv, dataset, table, write_disposition, check_duplicates, logger)
    elif choice == "3":
        table = "etmall_orders"
        csv = "data_processed/merged/etmall_orders_bq_formatted_20250807_115715.csv"
        logger.info(f"æº–å‚™ä¸Šå‚³: {csv} -> {dataset}.{table}")
        print(f"\næº–å‚™ä¸Šå‚³: {csv} -> {dataset}.{table}")
        upload_single_file(credential, csv, dataset, table, write_disposition, check_duplicates, logger)
    elif choice == "4":
        logger.info("é–‹å§‹ä¸Šå‚³æ‰€æœ‰æª”æ¡ˆ...")
        print("\nğŸš€ é–‹å§‹ä¸Šå‚³æ‰€æœ‰æª”æ¡ˆ...")
        for table_name, csv_path in DEFAULT_FILES.items():
            upload_single_file(credential, csv_path, dataset, table_name, write_disposition, check_duplicates, logger)
        logger.info("æ‰€æœ‰æª”æ¡ˆä¸Šå‚³å®Œæˆï¼")
        print("âœ… æ‰€æœ‰æª”æ¡ˆä¸Šå‚³å®Œæˆï¼")
    else:
        logger.warning(f"ç„¡æ•ˆé¸æ“‡: {choice}")
        print("âŒ ç„¡æ•ˆé¸æ“‡ï¼Œå·²å–æ¶ˆä¸Šå‚³ã€‚")


if __name__ == "__main__":
    # å¦‚æœå®Œå…¨æ²’å¸¶åƒæ•¸ï¼Œç›´æ¥é€²å…¥äº’å‹•å¼
    if len(sys.argv) == 1:
        interactive_mode()
    else:
        main()
